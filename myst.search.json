{"version":"1","records":[{"hierarchy":{"lvl1":"Example Viewer Template"},"type":"lvl1","url":"/","position":0},{"hierarchy":{"lvl1":"Example Viewer Template"},"content":"This template repository is intended to allow easy instantiation of an example viewer for jupyterlab notebooks.\nExternal repositories can be added via git submodules to the external_notebooks folder.\nThe github action will traverse available notebooks and try to extract metadata information as well as build them with Jupyterbook (v2 and MYST).\nThe build package is then deployed on github pages.","type":"content","url":"/","position":1},{"hierarchy":{"lvl1":"Evaluation of Fire Impact on Populated Areas on a European Site"},"type":"lvl1","url":"/notebooks/fire-impact-analysis","position":0},{"hierarchy":{"lvl1":"Evaluation of Fire Impact on Populated Areas on a European Site"},"content":"","type":"content","url":"/notebooks/fire-impact-analysis","position":1},{"hierarchy":{"lvl1":"Evaluation of Fire Impact on Populated Areas on a European Site"},"type":"lvl1","url":"/notebooks/fire-impact-analysis#evaluation-of-fire-impact-on-populated-areas-on-a-european-site","position":2},{"hierarchy":{"lvl1":"Evaluation of Fire Impact on Populated Areas on a European Site"},"content":"","type":"content","url":"/notebooks/fire-impact-analysis#evaluation-of-fire-impact-on-populated-areas-on-a-european-site","position":3},{"hierarchy":{"lvl1":"Evaluation of Fire Impact on Populated Areas on a European Site","lvl2":"Geo-storytelling with RACE, EO Dashboard and the Euro Data Cube"},"type":"lvl2","url":"/notebooks/fire-impact-analysis#geo-storytelling-with-race-eo-dashboard-and-the-euro-data-cube","position":4},{"hierarchy":{"lvl1":"Evaluation of Fire Impact on Populated Areas on a European Site","lvl2":"Geo-storytelling with RACE, EO Dashboard and the Euro Data Cube"},"content":"\n\nThis notebook demonstates how to access and analyse data found on the \n\nRapid Action for Citizens with Earth Observation (RACE) and \n\nEarth Observation Dashboard. It uses the EC-JRC’s Global Human Settlement (GHS) Layer which contains global data about the total built-up surface from 1975 to 2030 and was derived from Sentinel2 composite and Landsat imagery. The data is openly available and can be downloaded \n\nhere. It was ingested in EDC and provided as a layer, check out the documentation \n\nhere for further information about the data properties in EDC.\nThe GHS indicator can be explored on the EO Dashboard by selecting the \n\nEXPLORE DATASETS mode.\n\n","type":"content","url":"/notebooks/fire-impact-analysis#geo-storytelling-with-race-eo-dashboard-and-the-euro-data-cube","position":5},{"hierarchy":{"lvl1":"Evaluation of Fire Impact on Populated Areas on a European Site","lvl2":"Euro Data Cube - EDC Platform presentation"},"type":"lvl2","url":"/notebooks/fire-impact-analysis#euro-data-cube-edc-platform-presentation","position":6},{"hierarchy":{"lvl1":"Evaluation of Fire Impact on Populated Areas on a European Site","lvl2":"Euro Data Cube - EDC Platform presentation"},"content":"\n\n\n\n","type":"content","url":"/notebooks/fire-impact-analysis#euro-data-cube-edc-platform-presentation","position":7},{"hierarchy":{"lvl1":"Evaluation of Fire Impact on Populated Areas on a European Site","lvl2":"EDC - cloud-optimized platform offering:"},"type":"lvl2","url":"/notebooks/fire-impact-analysis#edc-cloud-optimized-platform-offering","position":8},{"hierarchy":{"lvl1":"Evaluation of Fire Impact on Populated Areas on a European Site","lvl2":"EDC - cloud-optimized platform offering:"},"content":"Access to EO archives from main all open missions (e.g. Sentinel, Landsat, MODIS, etc.), commercial satellites (PlanetScope, Pleiades, SPOT, WorldView, etc.) as well as Level 3 products (Copernicus Land Monitoring Services, C3S, etc.)\n\nAnalyse, compare and correlate EO data through Xcube and operational tools (Sentinel Hub)\n\nManage different data formats and type in a transparent way (raster, vector, COGs, Zarr, etc.)\n\nBring and store your own data and algorithm for real-time and batch processing operations\n\nComputational resources and storage to run Jupyter Notebooks and your deployed Applications within your Kubernetes-powered workspace\n\nExpose your apps on the EDC marketplace to third-parties and provide easy access to your managed API service to customers\n\n\n\n","type":"content","url":"/notebooks/fire-impact-analysis#edc-cloud-optimized-platform-offering","position":9},{"hierarchy":{"lvl1":"Evaluation of Fire Impact on Populated Areas on a European Site","lvl2":"EDC - Market Place"},"type":"lvl2","url":"/notebooks/fire-impact-analysis#edc-market-place","position":10},{"hierarchy":{"lvl1":"Evaluation of Fire Impact on Populated Areas on a European Site","lvl2":"EDC - Market Place"},"content":"\n\nData Products: e.g. \n\nSentinel Hub\n\nPlatform Services: e.g. \n\nEOxHub\n\nAPI Services: e.g. \n\nGeoDB, \n\nSH-Statistical API\n\n","type":"content","url":"/notebooks/fire-impact-analysis#edc-market-place","position":11},{"hierarchy":{"lvl1":"Content of this notebook:"},"type":"lvl1","url":"/notebooks/fire-impact-analysis#content-of-this-notebook","position":12},{"hierarchy":{"lvl1":"Content of this notebook:"},"content":"Visualizing wildfire events with Sentinel-2 imagery\n\nQuerying data via Statistical API for the chosen Area of Interest\n\nTransforming the output of the API request into a geoJSON polygon that contains the extent of the built up area in the AOI\n\nAccessing Sentinel-5p Carbon monoxide data from SentinelHub\n\nDisplaying the time series of the CO concentration over the populated area to evaluate the possible impact of the fire emissions\n\nThis notebook runs with the python environment users-edc-2023.07-01 and was prepared by Leah Sturm (University of Trier, Germany).\n\n#import necessary libraries\nimport os\nimport numpy as np\nimport geopandas as gpd\nimport rasterio\nfrom rasterio.features import shapes\nimport requests\nimport geojson\nfrom shapely.geometry import shape\nfrom rasterio.transform import from_origin\nimport matplotlib.pyplot as plt\nfrom datetime import datetime\n\n\n# Sentinel Hub requirements\nfrom sentinelhub import (SHConfig, DataCollection, Geometry, BBox, Geometry,\n                         SentinelHubRequest, filter_times, bbox_to_dimensions, MimeType, \n                         SentinelHubBYOC, ByocCollection, ByocTile, ByocCollectionAdditionalData,\n                         DownloadFailedException, CRS, SentinelHubStatistical)\n\n# Pass Sentinel Hub credentials to SHConfig\nconfig = SHConfig()\nconfig.sh_client_id = os.environ[\"SH_CLIENT_ID\"]\nconfig.sh_client_secret = os.environ[\"SH_CLIENT_SECRET\"]\n\nclient_id = os.environ[\"SH_CLIENT_ID\"]\nclient_secret = os.environ[\"SH_CLIENT_SECRET\"]\n\n\n# config\n\n","type":"content","url":"/notebooks/fire-impact-analysis#content-of-this-notebook","position":13},{"hierarchy":{"lvl1":"Content of this notebook:","lvl2":"Create a folder for the session in which all files will be stored"},"type":"lvl2","url":"/notebooks/fire-impact-analysis#create-a-folder-for-the-session-in-which-all-files-will-be-stored","position":14},{"hierarchy":{"lvl1":"Content of this notebook:","lvl2":"Create a folder for the session in which all files will be stored"},"content":"\n\n# Get the current date and format it as a string\ncurrent_date = datetime.now().strftime(\"%Y-%m-%d\")\n\n# Define the folder name and path\nfolder_name = f\"folder_{current_date}\"\nfolder_path = os.path.join(os.getcwd(), folder_name)\n\n# Check if the folder already exists, and create it if not\nif not os.path.exists(folder_path):\n    os.mkdir(folder_path)\n    print(f\"Folder '{folder_name}' created at: {folder_path}\")\nelse:\n    print(f\"Folder '{folder_name}' already exists at: {folder_path}\")\n\n","type":"content","url":"/notebooks/fire-impact-analysis#create-a-folder-for-the-session-in-which-all-files-will-be-stored","position":15},{"hierarchy":{"lvl1":"Wildfire Impacts and Carbon Emissions"},"type":"lvl1","url":"/notebooks/fire-impact-analysis#wildfire-impacts-and-carbon-emissions","position":16},{"hierarchy":{"lvl1":"Wildfire Impacts and Carbon Emissions"},"content":"The wildfire season during the summer of 2022 in Europe was exceptional, marked by a high number of observed fires, a large extent of burned area, and remarkably high atmospheric emissions linked to these fires. According to data from the European Forest Fire Information System (EFFIS), fires were reported in 26 out of the 27 European countries, collectively burning 837,212 hectares. A significant portion of these wildfires happened in July, with Spain, Portugal, France, and Italy experiencing the most damages.\n\nThe selected wildfire incident for this notebook occurred in the Gironde region of southwestern France, near the city of Bordeaux, in 2022. The significant fire event started on July 17, 2022, lasted for two weeks while burning approximately 7,000 hectares of land. Notably, the Copernicus Atmosphere Monitoring Service (CAMS) recorded exceptionally elevated levels of carbon monoxide emissions throughout the duration of this event.\n\nRelated articles about the event and the wildfire occurrence in 2022:\n\nEuropean Space Agency\n\nEU ScienceHub\n\nEFFIS\n\nCopernicus Atmosphere Monitoring Service\n\nEUMETSAT\n\n","type":"content","url":"/notebooks/fire-impact-analysis#wildfire-impacts-and-carbon-emissions","position":17},{"hierarchy":{"lvl1":"1. Visualize Wildfire Event with Sentinel-2"},"type":"lvl1","url":"/notebooks/fire-impact-analysis#id-1-visualize-wildfire-event-with-sentinel-2","position":18},{"hierarchy":{"lvl1":"1. Visualize Wildfire Event with Sentinel-2"},"content":"In this first part we are going to visualize the fire event to get an overview of the location and the occuring emissions. In the following cells we will access Sentinel-2 imagery from \n\nSentinel Hub. Sentinel Hub is a multi-spectral and multi-temporal big data satellite imagery service, capable of fully automated archiving, real-time processing and distribution of remote sensing data and related EO products. Users can use APIs to retrieve satellite data over their AOI and specific time range from full archives in a matter of seconds. The following cells to access Sentinel-2 data are based on the example notebook “Australian Bushfires” which is available in the \n\nEuro Data Cube Marketplace.\n\nTo get a first overview of the fire event we are going to look at the true colour image captured on the day of the start of the fire.\n\n","type":"content","url":"/notebooks/fire-impact-analysis#id-1-visualize-wildfire-event-with-sentinel-2","position":19},{"hierarchy":{"lvl1":"1. Visualize Wildfire Event with Sentinel-2","lvl2":"Define Area of Interest"},"type":"lvl2","url":"/notebooks/fire-impact-analysis#define-area-of-interest","position":20},{"hierarchy":{"lvl1":"1. Visualize Wildfire Event with Sentinel-2","lvl2":"Define Area of Interest"},"content":"\n\nbbox_coords =[\n  -1.006524,\n  44.318762,\n  -0.406214,\n  44.586411\n]\n\nresolution = 20\nArea_of_interest_bbox = BBox(bbox=bbox_coords, crs=CRS.WGS84)\nArea_of_interest_size = bbox_to_dimensions(Area_of_interest_bbox, resolution=resolution)\n\n# Bbox EPSG\nbbox_epsg = 4326\n\n# Utilities\nimport IPython.display\nfrom IPython.display import display, GeoJSON\n\n# Plot the bounding box on a map\nIPython.display.GeoJSON(BBox(bbox_coords,crs=bbox_epsg).get_geojson())\n\n","type":"content","url":"/notebooks/fire-impact-analysis#define-area-of-interest","position":21},{"hierarchy":{"lvl1":"1. Visualize Wildfire Event with Sentinel-2","lvl2":"Access Sentinel-2 data"},"type":"lvl2","url":"/notebooks/fire-impact-analysis#access-sentinel-2-data","position":22},{"hierarchy":{"lvl1":"1. Visualize Wildfire Event with Sentinel-2","lvl2":"Access Sentinel-2 data"},"content":"We build the request according to the \n\nAPI Reference, using the SentinelHubRequest class. Each Process API request also needs an \n\nevalscript.\n\nThe information that we specify in the SentinelHubRequest object is:\n\nan evalscript,\n\na list of input data collections with time interval,\n\na format of the response,\n\na bounding box and it’s size (size or resolution).\n\nThe evalscript is used to select the appropriate bands. We return the RGB (B04, B03, B02) Sentinel-2 L2A bands.\n\n# define the evalscript \n\nevalscript_true_color = \"\"\"\n    //VERSION=3\n\n    function setup() {\n        return {\n            input: [{\n                bands: [\"B02\", \"B03\", \"B04\"]\n            }],\n            output: {\n                bands: 3\n            }\n        };\n    }\n\n    function evaluatePixel(sample) {\n        return [sample.B04, sample.B03, sample.B02];\n    }\n\"\"\"\n\n# we nned to specify the collection and the time interval\n# for a list of collections available in the Sentinel Hub visit https://docs.sentinel-hub.com/api/latest/data/ \n\nrequest_true_color = SentinelHubRequest(\n    evalscript=evalscript_true_color,\n    input_data=[\n        SentinelHubRequest.input_data(\n            data_collection=DataCollection.SENTINEL2_L2A, \n            time_interval=('2022-07-16', '2022-07-18'),\n        )\n    ],\n    responses=[\n        SentinelHubRequest.output_response('default', MimeType.PNG)\n    ],\n    bbox=Area_of_interest_bbox,\n    size=Area_of_interest_size,\n    config=config\n)\n\n","type":"content","url":"/notebooks/fire-impact-analysis#access-sentinel-2-data","position":23},{"hierarchy":{"lvl1":"1. Visualize Wildfire Event with Sentinel-2","lvl2":"Visualize the requested Sentinel image"},"type":"lvl2","url":"/notebooks/fire-impact-analysis#visualize-the-requested-sentinel-image","position":24},{"hierarchy":{"lvl1":"1. Visualize Wildfire Event with Sentinel-2","lvl2":"Visualize the requested Sentinel image"},"content":"\n\n# send the request to Sentinel Hub\n#true_color_imgs = request_true_color.get_data()\n\nmax_retries = 3\nretry_count = 0\nwhile retry_count < max_retries:\n    try:\n        true_color_imgs = request_true_color.get_data()\n        break  # Image displayed successfully, exit the loop\n    except Exception as e:\n        print(f\"Failed to display image: {e}\")\n        retry_count += 1\n        if retry_count < max_retries:\n            print(f\"Retrying (Attempt {retry_count})...\")\n \nif retry_count >= max_retries:\n    print(\"Maximum retries reached. Unable to retrieve the image.\")\n    \n# Define the plot_image function\n\ndef plot_image(image, factor=1.0, clip_range = None, **kwargs):\n    \"\"\"\n    Utility function for plotting RGB images.\n    \"\"\"\n    fig, ax = plt.subplots(nrows=1, ncols=1, figsize=(15, 15))\n    if clip_range is not None:\n        ax.imshow(np.clip(image * factor, *clip_range), **kwargs)\n    else:\n        ax.imshow(image * factor, **kwargs)\n    ax.set_xticks([])\n    ax.set_yticks([])\n\nimage = true_color_imgs[0]\nprint(f'Image type: {image.dtype}')\n    \n# plot function\n# factor 1/255 to scale between 0-1\n# factor 3.5 to increase brightness\n\n# Maximum number of retry attempts\n\nplot_image(image, factor=3.5/255, clip_range=(0,1))\n\nThe visualized image captures the start of the  wildfire event which is prominently visible in the top right corner. The plume and emissions from the fire event extend westwards. Consequently, the analysis within this notebook will center around urban areas within those western regions.\n\n","type":"content","url":"/notebooks/fire-impact-analysis#visualize-the-requested-sentinel-image","position":25},{"hierarchy":{"lvl1":"1. Visualize Wildfire Event with Sentinel-2","lvl2":"Define and visualize AOI for the analysis"},"type":"lvl2","url":"/notebooks/fire-impact-analysis#define-and-visualize-aoi-for-the-analysis","position":26},{"hierarchy":{"lvl1":"1. Visualize Wildfire Event with Sentinel-2","lvl2":"Define and visualize AOI for the analysis"},"content":"\n\n# define the coordinates\ntop_left_x =  -1.26747\ntop_left_y = 44.674299\nbottom_right_x = -0.962647\nbottom_right_y = 44.508045\n\nbbox = [   \n  top_left_x,\n  bottom_right_y,\n  bottom_right_x,\n  top_left_y\n        ]\n\n# Bbox EPSG\nbbox_epsg = 4326\n\n# Plot the bounding box on a map\nIPython.display.GeoJSON(BBox(bbox,crs=bbox_epsg).get_geojson())\n\n","type":"content","url":"/notebooks/fire-impact-analysis#define-and-visualize-aoi-for-the-analysis","position":27},{"hierarchy":{"lvl1":"1. Visualize Wildfire Event with Sentinel-2","lvl2":"Check the resolution and size of the AOI"},"type":"lvl2","url":"/notebooks/fire-impact-analysis#check-the-resolution-and-size-of-the-aoi","position":28},{"hierarchy":{"lvl1":"1. Visualize Wildfire Event with Sentinel-2","lvl2":"Check the resolution and size of the AOI"},"content":"\n\n# Check the resolution and pixels constrains (they have to be maximal 2500x2500)\nresolution = 10\naoi = BBox(bbox=bbox, crs=CRS.WGS84)\naoi_size = bbox_to_dimensions(aoi, resolution=resolution)\n\n# These values are needed to set the right dimensions for saving the request as tiff file later\nwidth = aoi_size[0]  \nheight = aoi_size[1] \n\nprint(f\"Image shape at {resolution} m resolution: {aoi_size} pixels\")\n\n","type":"content","url":"/notebooks/fire-impact-analysis#check-the-resolution-and-size-of-the-aoi","position":29},{"hierarchy":{"lvl1":"2. Query GHS population density layer with SentinelHub API request"},"type":"lvl1","url":"/notebooks/fire-impact-analysis#id-2-query-ghs-population-density-layer-with-sentinelhub-api-request","position":30},{"hierarchy":{"lvl1":"2. Query GHS population density layer with SentinelHub API request"},"content":"The \n\nStatistical API empowers users to derive insightful statistics from satellite imagery, eliminating the need to download large image files. When making a Statistical API request,it is possible to define an AOI, time frame, evalscript, and the specific statistical measures you wish to compute. The resulting statistics are conveniently included in the API response. With the Statistical API, the user can compute statistics such as cloud pixel percentages within a defined area and timeframe or calculate metrics like mean, standard deviation, and histogram of band values for a specific parcel over a given time frame.\n\nTo access the population density layer, we need to use the designated BYOC ID: 0c7aa265-50f9-4947-9980-2ee5ae204803 found on EDC. To query the GHS layer effectively, we need to provide the BYOC ID, the coordinates from the created GeoJSON file as AOI, the desired timeframe, and the necessary user credentials as inputs. The GHS layer contains data from 1975 to 2030 in 5 years intervals. We are going to query the data for 2020 because it is temporally closest to the chosen wildfire event in 2022.\n\nSource for EC-JRC’s GHS layer: Pesaresi, Martino; Politis, Panagiotis (2023): GHS-BUILT-S R2023A - GHS built-up surface grid, derived from Sentinel2 composite and Landsat, multitemporal (1975-2030). European Commission, Joint Research Centre (JRC) [Dataset] doi: 10.2905/9F06F36F-4B11-47EC-ABB0-4F8B7B1D72EA\n\nThe evalscript in this request creates a new output image where the “dataMask” band is modified based on the values in the GHS layer. If the GHS value is 0, it masks the pixel in the “dataMask” band by setting it to zero. Otherwise, it retains the original GHS value.\n\npopulation_dens = \"\"\"\n//VERSION=3\nfunction setup() {\n  return {\n    input: [{\n      bands: [\"BUILT\", \"dataMask\"], // this sets which bands to use\n    }],\n    output: { // this defines the output image type\n      bands: 1,\n      sampleType: \"UINT8\"\n    }\n  };\n}\n \nfunction evaluatePixel(sample) {\n    let pixelMask = 1\n    \n    if (sample.BUILT == 0){\n        pixelMask = 0\n    }\n  return {\n    default: [sample.BUILT],\n    dataMask: [sample.dataMask * pixelMask]\n  };\n}\n\"\"\"\n\nrequest_data = SentinelHubRequest(\n    evalscript=population_dens,\n    input_data=[\n        SentinelHubRequest.input_data(\n            data_collection=DataCollection.define_byoc('0c7aa265-50f9-4947-9980-2ee5ae204803'),\n            time_interval=(\"2020-01-01\", \"2020-01-01\"),\n        )\n    ],\n    responses=[SentinelHubRequest.output_response(\"default\", MimeType.PNG)],\n    bbox=aoi,\n    size=aoi_size,\n    config=config,\n)\n\npopulation_density = request_data.get_data()\n\n","type":"content","url":"/notebooks/fire-impact-analysis#id-2-query-ghs-population-density-layer-with-sentinelhub-api-request","position":31},{"hierarchy":{"lvl1":"2. Query GHS population density layer with SentinelHub API request","lvl2":"Extract the population density extent as GeoTiff file"},"type":"lvl2","url":"/notebooks/fire-impact-analysis#extract-the-population-density-extent-as-geotiff-file","position":32},{"hierarchy":{"lvl1":"2. Query GHS population density layer with SentinelHub API request","lvl2":"Extract the population density extent as GeoTiff file"},"content":"The GHS layer contains \n\nvalues ranging from 0 to 10,000, where higher values indicate greater population density. In this notebook, we only exclude pixels with zero values, but I would be possible to add a threshold to the script below to pinpoint areas of exceptionally high population density.\n\n# the final geotiff is a binary mask with values of 1 representing built-up pixels\nimage_data_list = population_density\n\n# Calculate pixel width and height based on image shape\npixel_width = (bottom_right_x - top_left_x) /  width \npixel_height = (top_left_y - bottom_right_y) / height\n\n# Specify the path to the directory where GeoTIFF files will be saved\noutput_directory = f\"./{folder_name}\"\n# Loop through the list of image arrays\nfor i, image_data in enumerate(image_data_list):\n    # Extract the pixel values (assuming single-band data)\n    pixel_values = image_data[:, :]\n\n    # Set values equal to zero to zero, and all other values to 1\n    pixel_values = np.where(pixel_values == 0, 0, 1)\n\n    # Specify the output path for each GeoTIFF file with the current time\n    output_path = os.path.join(output_directory, f\"output_{current_date}.tif\")\n\n    # Create a transformation for the GeoTIFF\n    transform = from_origin(top_left_x, top_left_y, pixel_width, pixel_height)\n\n    # Open a new GeoTIFF file for writing with NoData value set to NaN\n    with rasterio.open(\n        output_path,\n        'w',\n        driver='GTiff',\n        height=pixel_values.shape[0],\n        width=pixel_values.shape[1],\n        count=1,  # Only one band for pixel values\n        dtype=rasterio.float32,  # Use float32 for NaN values\n        crs='EPSG:4326',\n        transform=transform,\n        nodata=np.nan  # Set NoData value to NaN\n    ) as dst:\n        # Write the pixel values to the GeoTIFF\n        dst.write(pixel_values, 1)  # Use band 1\n\n\n","type":"content","url":"/notebooks/fire-impact-analysis#extract-the-population-density-extent-as-geotiff-file","position":33},{"hierarchy":{"lvl1":"2. Query GHS population density layer with SentinelHub API request","lvl2":"Visualize the extent of the population density layer"},"type":"lvl2","url":"/notebooks/fire-impact-analysis#visualize-the-extent-of-the-population-density-layer","position":34},{"hierarchy":{"lvl1":"2. Query GHS population density layer with SentinelHub API request","lvl2":"Visualize the extent of the population density layer"},"content":"\n\n# Specify the path to the GeoTIFF file\ngeotiff_path = f\"./{folder_name}/output_{current_date}.tif\"\n\n# Open the GeoTIFF file using rasterio\nwith rasterio.open(geotiff_path) as src:\n    # Read the raster data\n    raster_data = src.read(1)  # Assuming you have a single band GeoTIFF\n\n    # Get the spatial transformation information\n    transform = src.transform\n\n# Calculate the bounds based on the width and height of the raster\nleft, bottom, right, top = src.bounds\n\n# Plot the GeoTIFF data using matplotlib\nplt.figure(figsize=(8, 8))\nplt.imshow(raster_data, cmap='gray', extent=(left, right, bottom, top), origin='upper')\nplt.title('Population density in AOI')\nplt.xlabel('Longitude')\nplt.ylabel('Latitude')\nplt.grid(True)\nplt.show()\n\nAll non populated areas are displayed in black in the plot while the populated areas are colored white. From this binary mask we can now create a GeoJSON file that contains only the populated areas.\n\n","type":"content","url":"/notebooks/fire-impact-analysis#visualize-the-extent-of-the-population-density-layer","position":35},{"hierarchy":{"lvl1":"2. Query GHS population density layer with SentinelHub API request","lvl2":"3. Convert the binary raster mask into a GeoJSON file"},"type":"lvl2","url":"/notebooks/fire-impact-analysis#id-3-convert-the-binary-raster-mask-into-a-geojson-file","position":36},{"hierarchy":{"lvl1":"2. Query GHS population density layer with SentinelHub API request","lvl2":"3. Convert the binary raster mask into a GeoJSON file"},"content":"The resulting GeoJSON will only contain polygons for built up areas and will be used as spatial extend to query the Sentinel-5p data later.\n\n# Specify the path to the GeoTIFF file\ngeotiff_path = f\"./{folder_name}/output_{current_date}.tif\"\n\n# Open the GeoTIFF file\nwith rasterio.open(geotiff_path) as src:\n    # Read the binary mask data\n    mask = src.read(1)\n\n# Convert the binary mask to vector polygons\ngeoms = list(shapes(mask, transform=src.transform, connectivity=4))  # Specify connectivity=4 for 4-connected pixels\n\n# Filter the polygons to include only those corresponding to pixels with a value of 1\nfiltered_geoms = [geom for geom, value in geoms if value == 1]\n\n# Create a GeoDataFrame from the filtered polygons\ngdf = gpd.GeoDataFrame({'geometry': [shape(geom) for geom in filtered_geoms]})\n\n# Merge all the geometries into a single MultiPolygon\nmulti_polygon = gdf.unary_union\n\n# Create a GeoDataFrame with the MultiPolygon geometry\nmulti_polygon_gdf = gpd.GeoDataFrame(geometry=[multi_polygon], crs=gdf.crs)\n\n# Specify the path to save the MultiPolygon GeoJSON file\noutput_geojson_file = f\"./{folder_name}/multi_polygon_{current_date}.geojson\"\n\n# Save the GeoDataFrame with the MultiPolygon to a GeoJSON file\nmulti_polygon_gdf.to_file(output_geojson_file, driver='GeoJSON')\n\n","type":"content","url":"/notebooks/fire-impact-analysis#id-3-convert-the-binary-raster-mask-into-a-geojson-file","position":37},{"hierarchy":{"lvl1":"2. Query GHS population density layer with SentinelHub API request","lvl2":"Plot the GeoJSON file"},"type":"lvl2","url":"/notebooks/fire-impact-analysis#plot-the-geojson-file","position":38},{"hierarchy":{"lvl1":"2. Query GHS population density layer with SentinelHub API request","lvl2":"Plot the GeoJSON file"},"content":"\n\ndisplay(GeoJSON(data=f\"./{folder_name}/multi_polygon_{current_date}.geojson\", crs=bbox_epsg))\n\nWe can see that now only the built-up areas are stored in the GeoJSON file and we can extract the coordinates from this and use them as an input for the Sentinel-5p data\n\n","type":"content","url":"/notebooks/fire-impact-analysis#plot-the-geojson-file","position":39},{"hierarchy":{"lvl1":"2. Query GHS population density layer with SentinelHub API request","lvl2":"Extract the coordinates of the created GeoJSON file"},"type":"lvl2","url":"/notebooks/fire-impact-analysis#extract-the-coordinates-of-the-created-geojson-file","position":40},{"hierarchy":{"lvl1":"2. Query GHS population density layer with SentinelHub API request","lvl2":"Extract the coordinates of the created GeoJSON file"},"content":"\n\nwith open(f\"./{folder_name}/multi_polygon_{current_date}.geojson\") as f:\n    gj = geojson.load(f)\ndata_coordinates = gj['features'][0]['geometry']['coordinates']\n\n#print(data_coordinates)\n\n","type":"content","url":"/notebooks/fire-impact-analysis#extract-the-coordinates-of-the-created-geojson-file","position":41},{"hierarchy":{"lvl1":"4. Use Statistical API request to access Sentinel-5p CO data for the extracted populated areas"},"type":"lvl1","url":"/notebooks/fire-impact-analysis#id-4-use-statistical-api-request-to-access-sentinel-5p-co-data-for-the-extracted-populated-areas","position":42},{"hierarchy":{"lvl1":"4. Use Statistical API request to access Sentinel-5p CO data for the extracted populated areas"},"content":"\n\nfrom oauthlib.oauth2 import BackendApplicationClient\nfrom requests_oauthlib import OAuth2Session\n\n# Create a session\nclient = BackendApplicationClient(client_id=client_id)\noauth = OAuth2Session(client=client)\n\n# Get token for the session\ntoken = oauth.fetch_token(token_url='https://services.sentinel-hub.com/oauth/token',\n                          client_secret=client_secret)\n\n# All requests using this session will have an access token automatically added\nresp = oauth.get(\"https://services.sentinel-hub.com/oauth/tokeninfo\")\n\nurl = \"https://creodias.sentinel-hub.com/api/v1/statistics\"\nheaders = {\n  \"Accept\": \"application/json\",\n  \"Content-Type\": \"application/json\"\n}\ndata = {\n  \"input\": {\n    \"bounds\": {\n      \"geometry\": {\n        \"type\": \"MultiPolygon\",\n        \"coordinates\": data_coordinates\n                 }\n              },\n    \"data\": [\n      {\n        \"dataFilter\": {},\n        \"type\": \"sentinel-5p-l2\"\n      }\n    ]\n  },\n  \"aggregation\": {\n    \"timeRange\": {\n      \"from\": \"2022-07-01T00:00:00Z\",\n      \"to\": \"2022-08-09T23:59:59Z\"\n    },\n    \"aggregationInterval\": {\n      \"of\": \"P1D\"\n    },\n    \"width\": 512,\n    \"height\": 402.581,\n    \"evalscript\": \"//VERSION=3\\nfunction setup() {\\n  return {\\n    input: [{\\n      bands: [\\\"CO\\\", \\\"dataMask\\\"], // this sets which bands to use\\n    }],\\n    output: [\\n      { id:\\\"default\\\", bands: 1, sampleType: \\\"FLOAT32\\\" },\\n      { id: \\\"dataMask\\\", bands: 1 }\\n    ]\\n  };\\n}\\n \\n\\n\\nfunction evaluatePixel(sample) {\\n  return {\\n    default: [sample.CO],\\n    dataMask: [sample.dataMask]\\n  };\\n}\"\n  },\n  \"calculations\": {\n    \"default\": {}\n  }\n}\n\nresponse = oauth.post(url, headers=headers, json=data)\n\n","type":"content","url":"/notebooks/fire-impact-analysis#id-4-use-statistical-api-request-to-access-sentinel-5p-co-data-for-the-extracted-populated-areas","position":43},{"hierarchy":{"lvl1":"4. Use Statistical API request to access Sentinel-5p CO data for the extracted populated areas","lvl2":"Save the extracted statistics"},"type":"lvl2","url":"/notebooks/fire-impact-analysis#save-the-extracted-statistics","position":44},{"hierarchy":{"lvl1":"4. Use Statistical API request to access Sentinel-5p CO data for the extracted populated areas","lvl2":"Save the extracted statistics"},"content":"\n\nresponse_data = response.json()\n#print(response_data)\n\n","type":"content","url":"/notebooks/fire-impact-analysis#save-the-extracted-statistics","position":45},{"hierarchy":{"lvl1":"5. Plot the statistics for the S5p CO data"},"type":"lvl1","url":"/notebooks/fire-impact-analysis#id-5-plot-the-statistics-for-the-s5p-co-data","position":46},{"hierarchy":{"lvl1":"5. Plot the statistics for the S5p CO data"},"content":"\n\ntime_axis = [datetime.strptime(entry[\"interval\"][\"from\"], \"%Y-%m-%dT%H:%M:%SZ\").strftime(\"%Y/%m/%d\") for entry in response_data[\"data\"]]\nmeans = [float(entry['outputs']['default']['bands']['B0']['stats']['mean']) for entry in response_data[\"data\"]]\nstd_devs = [float(entry['outputs']['default']['bands']['B0']['stats']['stDev']) for entry in response_data[\"data\"]]\nstd_upper = [m+s for (m,s) in zip(means, std_devs)]\nstd_lower = [m-s for (m,s) in zip(means, std_devs)]\n\nfig = plt.figure(figsize=(20,10))\n# plot mean values \nplt.plot_date(time_axis, means, linestyle='solid', linewidth=2, color=\"black\", label=\"Mean CO values [mol/ m^2]\")\n\n# plot standard deviation error bars\nplt.errorbar(time_axis, means, yerr=std_devs, linestyle=\"-\")\n\nplt.title('CO Values over the AOI (07/2022 - 08/2022)')\nplt.legend()\nfig.autofmt_xdate()\nplt.show()\n\nThe graph illustrates a noticeable increase in CO concentration across populated areas on July 17th, when the fire event started. Additionally, the Standard Deviation on this day and the following days is increased in comparison to the days before the fire. The highest \n\ncarbon emissions in France were recorded from June to August in 2022 which aligns well with the result of this analysis. To learn more about the carbon emissions resulting from wildfires check out this \n\nstory on the EO Dashboard that also incorporates further indicators to analyse wildfires.","type":"content","url":"/notebooks/fire-impact-analysis#id-5-plot-the-statistics-for-the-s5p-co-data","position":47},{"hierarchy":{"lvl1":"Exploring Inland Water Bodies with Sentinel-2 data in EDC"},"type":"lvl1","url":"/notebooks/inland-water-with-edc","position":0},{"hierarchy":{"lvl1":"Exploring Inland Water Bodies with Sentinel-2 data in EDC"},"content":"from edc import check_compatibility\ncheck_compatibility(\"user-2023.03-02\", dependencies=[\"SH\"])\n\n","type":"content","url":"/notebooks/inland-water-with-edc","position":1},{"hierarchy":{"lvl1":"Exploring Inland Water Bodies with Sentinel-2 data in EDC"},"type":"lvl1","url":"/notebooks/inland-water-with-edc#exploring-inland-water-bodies-with-sentinel-2-data-in-edc","position":2},{"hierarchy":{"lvl1":"Exploring Inland Water Bodies with Sentinel-2 data in EDC"},"content":"\n\nauthor: Anca Anghelea, based on a \n\nnotebook by: William Ray\n\nSeveral lakes and other inland water bodies are featured on EO Dashboard. The datasets supporting the various geo-stories are accessible by means similar to what you will learn in this notebook.\n\nExplore EO Dashboard Stories on Oceans and Inland Water.","type":"content","url":"/notebooks/inland-water-with-edc#exploring-inland-water-bodies-with-sentinel-2-data-in-edc","position":3},{"hierarchy":{"lvl1":"Exploring Inland Water Bodies with Sentinel-2 data in EDC","lvl2":"In this notebook"},"type":"lvl2","url":"/notebooks/inland-water-with-edc#in-this-notebook","position":4},{"hierarchy":{"lvl1":"Exploring Inland Water Bodies with Sentinel-2 data in EDC","lvl2":"In this notebook"},"content":"In this demonstration Jupyter Notebook, we will be visualising and analysing inland water bodies using Sentinel data, demonstrating the use of EDC.\n\nWe are going to use the EDC and its associated libaries and APIs to do this. In this notebook we will learn how to:\n\nBuild a cube\n\nVisualise a variable in your data cube\n\nCreate a new variable\n\nCreate a new variable using a threshold\n\nVisualise a spatial subset of a variable over time\n\nCreate a new variable based upon space and time.\n\n","type":"content","url":"/notebooks/inland-water-with-edc#in-this-notebook","position":5},{"hierarchy":{"lvl1":"Exploring Inland Water Bodies with Sentinel-2 data in EDC","lvl2":"Configuration"},"type":"lvl2","url":"/notebooks/inland-water-with-edc#configuration","position":6},{"hierarchy":{"lvl1":"Exploring Inland Water Bodies with Sentinel-2 data in EDC","lvl2":"Configuration"},"content":"Before acccessing the data, we will start by importing the necessary Python libraries (already configured in your EDC workspace), and generate credentials automatically to access the services.\n\n# EDC libraries\nfrom edc import setup_environment_variables\nfrom xcube_sh.config import CubeConfig\nfrom xcube_sh.cube import open_cube\nfrom xcube_sh.sentinelhub import SentinelHub\nfrom xcube.core.gen2.local.combiner import CubesCombiner\nfrom xcube.core.geom import mask_dataset_by_geometry\n\n# Sentinel Hub\nfrom sentinelhub import BBox, SentinelHubRequest, bbox_to_dimensions, DataCollection, MimeType, SHConfig, geometry\n\n# Utilities\nimport IPython.display\nfrom os import environ\nimport matplotlib.pyplot as plt\nimport datetime as dt\nimport geopandas\nimport rioxarray\n\n# Numerical computation\nimport xarray as xr\nimport numpy as np\n\n# Fetch credentials as environement variables\nsetup_environment_variables()\n\n# Pass Sentinel Hub credentials to dictionnary\nsh_credentials = dict(client_id=environ[\"SH_CLIENT_ID\"],\n                      client_secret=environ[\"SH_CLIENT_SECRET\"])\n\nDefine an AOI\n\nNext, we will define our area of interest using a bounding box. This must be provided in WGS84 coordinates to build the cube.\n\nWe have chosen an AOI covering the natural park “Valli di Comacchio” in Italy.\n\n# Define the coordinates of the bounding box\nlake_bbox = [12.09, 44.54, 12.27, 44.70]\n\n# Bbox EPSG\nbbox_epsg = 4326\n\n\n# Plot the bounding box on a map\nIPython.display.GeoJSON(BBox(lake_bbox,crs=bbox_epsg).get_geojson())\n\n","type":"content","url":"/notebooks/inland-water-with-edc#configuration","position":7},{"hierarchy":{"lvl1":"Exploring Inland Water Bodies with Sentinel-2 data in EDC","lvl3":"How to build a data cube","lvl2":"Configuration"},"type":"lvl3","url":"/notebooks/inland-water-with-edc#how-to-build-a-data-cube","position":8},{"hierarchy":{"lvl1":"Exploring Inland Water Bodies with Sentinel-2 data in EDC","lvl3":"How to build a data cube","lvl2":"Configuration"},"content":"Firstly, we will go through how to build a data cube.\n\nWe are going to visualise the floods using Sentinel-2 imagery. Sentinel-2 is part of the Copernicus programme and collects multispectral data globally with a revisit time of 5 days. The satellite’s multispectral imager provides collects data in 13 spectral bands spanning from the visible and near infrared to the shortwave infrared. The visible and near infrared data we will use in this example is collected at 10m resolution.\n\nCheck Sentinel-2 L2A available bands\n\nUsing EDC inbuilt functions that query Sentinel Hub services, we can easily list the available bands for a given dataset to help us build the cube!\n\n# Create a Sentinel Hub class, using our Sentinel Hub credentials\nSH = SentinelHub(**sh_credentials)\n\n# List bands for S2-L2A\nSH.band_names('S2L2A')\n\nBuild an xcube\n\nIn the following cell we will specify the input parameters needed to build an xcube array. The following parameters are specified:\n\ndataset_name: the Sentinel Hub identification of the dataset. Here we will call S2L2A for Sentinel-2 L2A. All available datasets can be listed with SH.dataset_names\n\nband_names: the band names to be used in the xcube array (see previous code cell). Here, we will call the B02, B03, B04, B08, CLM (Blue, Green, Red, NIR, Cloud Mask) bands.\n\nbbox: the bounding box that sets the extent of the AOI. Because we are using the default WGS84 coordinate system here, the CRS parameter doesn’t need to be set.\n\nspatial_res: the spatial resolution of the rasters contained in the xcube array. The spatial resolution is expressed in the units of the coordinate system used. Therefore, in this example, the spatial resolution is set in degrees. For an approximate pixel size of 10 meters, we set the resolution to 0.000089 degrees.\n\ntime_range: a list of two dates [start_date, end_date] forming a time period for which all acquisitions will be returned. Sentinel-2 L2A data is available from October 2016 onwards. In this example, we will fetch data for June 2023 - mid July 2023.\n\ntime_tolerance: The tolerance used to identify whether a dataset should still be included within a time period. Here, 30m corresponds to 30 minutes, thus avoiding duplicate datasets.\n\n# Setup xcube\ns2_cube_config = CubeConfig(dataset_name='S2L2A',\n                         band_names=['B02', 'B03', 'B04', 'B08', 'CLM'],\n                         bbox=lake_bbox,\n                         spatial_res=0.000089,\n                         time_range=['2023-06-01', '2023-07-16'],\n                         time_tolerance='30m')\n\nOpen the xcube\n\nIn the following cell we open the cube and display its contents. The automatically generated credentials obtained earlier in this Jupyter Notebook are specified as a parameter when opening the cube. It’s important to note that at this stage, we’re not processing anything, just generating a cube on the fly with data ready to be called when needed for analysis.\n\nOnce you open the cube, you can visualise the contents. You can view the number of timestamps and a list of them all too in the Coordinates tab. You can also visualise the seperate variables, with information on the size of the variables and their data type too.\n\n# Open cube (on the fly)\ns2_cube = open_cube(s2_cube_config, **sh_credentials)\n\n# Display contents\ns2_cube\n\n","type":"content","url":"/notebooks/inland-water-with-edc#how-to-build-a-data-cube","position":9},{"hierarchy":{"lvl1":"Exploring Inland Water Bodies with Sentinel-2 data in EDC","lvl3":"How to visualise your datacube","lvl2":"Configuration"},"type":"lvl3","url":"/notebooks/inland-water-with-edc#how-to-visualise-your-datacube","position":10},{"hierarchy":{"lvl1":"Exploring Inland Water Bodies with Sentinel-2 data in EDC","lvl3":"How to visualise your datacube","lvl2":"Configuration"},"content":"Now we have built our cube, let’s visualise the data! We are going to visualise a True Color image and an NDWI image in the same plot. In the below cell you can see we are selecting each band for 10:00:00 16th June 2023, and selecting the nearest acquisition to this date and time. We then stack the three bands and plot this using Matplotlib. We will call the three bands in the visible spectrum. In addition we will multiply the reflectance values by 5 to brighten the image.\n\nAnother way to visualise the extent of surface water is to use the Normalised Difference Water Index (NDWI). This is an index that can be used to extract surface water using multispectral imagery such as Sentinel-2. We can calculate the index with the Green and NIR bands as stated below, and add it into the data cube as a new variable.\n\nNDWI = Green - NIR / Green + NIR\n\nFor this we are going to create a new variable in the next cell. To create the new variable we are using two existing variables defined as s2_cube.B03 and s2_cube.B08. We then insert these variables into an index formula to create NDWI. Once ndwi has been calculated it’s attributed a long_name and units before being defined as ndwi so that we can call it as a definition later in the notebook.\n\n# Define NDWI in visualisation\nndwi = ((s2_cube.B03-s2_cube.B08)/(s2_cube.B03+s2_cube.B08))\n\nndwi.attrs['long_name']='NDWI'\nndwi.attrs['units']='unitless'\n\ns2_cube['NDWI']= ndwi  \n\nNext we want to plot both the True Color image and the NDWI in the same plot. We will use Matplotlib to achieve this.\n\n# Select the bands and stack them.\nRed = s2_cube.B04.sel(time='2023-06-17 10:00:00', method='nearest')\nGreen = s2_cube.B03.sel(time='2023-06-17 10:00:00', method='nearest')\nBlue = s2_cube.B02.sel(time='2023-06-17 10:00:00', method='nearest')\n\nrgb = np.dstack((Red,Green,Blue)) #Stack the three arrays\n\nndwi = s2_cube.NDWI.sel(time='2023-06-17 10:00:00', method='nearest')\n\n# Plot \nf = plt.figure(figsize=[10, 15])\nf.add_subplot(1, 2, 1)\nplt.title(f\"True Color: {str(s2_cube.time.sel(time='2023-06-17 10:00:00', method='nearest').data).split('T')[0]}\")\nplt.imshow(5 * rgb)  # We multiply the rgb by 5 to make the image brighter\nf.add_subplot(1, 2, 2)\nplt.title(f\"NDWI: {str(s2_cube.time.sel(time='2023-06-17 10:00:00', method='nearest').data).split('T')[0]}\")\nplt.imshow(ndwi, vmin=-1, vmax=1, cmap='GnBu')\nplt.show()\n\nThis looks good, and the extent of the flood waters is visualised really nicely here. The 10m resolution also enables us to see individual fields around the lake with the linear boundaries of the fields highlighted nicely in the high resolution image provided by the 10m Sentinel 2 bands.\n\nLet’s try and visualise some more dates in the time period that we are examining;\n\n# Select timestamps\nndwi1 = s2_cube.NDWI.sel(time='2023-06-17 10:00:00', method='nearest')\nndwi2 = s2_cube.NDWI.sel(time='2023-06-24 10:00:00', method='nearest')\nndwi3 = s2_cube.NDWI.sel(time='2023-06-29 10:00:00', method='nearest')\nndwi4 = s2_cube.NDWI.sel(time='2023-07-04 10:00:00', method='nearest')\nndwi5 = s2_cube.NDWI.sel(time='2023-07-07 10:00:00', method='nearest')\nndwi6 = s2_cube.NDWI.sel(time='2023-07-09 10:00:00', method='nearest')\n\n\n# Plot \nf = plt.figure(figsize=[15,11])\nax1 = f.add_subplot(2,3, 1)\nax2 = f.add_subplot(2,3, 2)\nax3 = f.add_subplot(2,3, 3)\nax4 = f.add_subplot(2,3, 4)\nax5 = f.add_subplot(2,3, 5)\nax6 = f.add_subplot(2,3, 6)\n\naxlist=[ax1,ax2,ax3,ax4,ax5,ax6]\n\nt = ndwi1.plot.imshow(ax=ax1, vmin=-1, vmax=1, cmap='GnBu', add_colorbar=False)\nndwi2.plot.imshow(ax=ax2, vmin=-1, vmax=1, cmap='GnBu', add_colorbar=False)\nndwi3.plot.imshow(ax=ax3, vmin=-1, vmax=1, cmap='GnBu', add_colorbar=False)\nndwi4.plot.imshow(ax=ax4, vmin=-1, vmax=1, cmap='GnBu', add_colorbar=False)\nndwi5.plot.imshow(ax=ax5, vmin=-1, vmax=1, cmap='GnBu', add_colorbar=False)\nndwi6.plot.imshow(ax=ax6, vmin=-1, vmax=1, cmap='GnBu', add_colorbar=False)\n\ncbar_ax = f.add_axes([1, 0.15, 0.05, 0.7])\nf.colorbar(t, cax=cbar_ax, label=\"NDWI\")\n\n#we will save the output image so we need to ensure that it is fully rendered \nplt.tight_layout() \n\n# Save the figure to a PNG file\nplt.savefig('NDWI.png')\n\nplt.show()\n\nWe could use the NDWI to estimate the surface water extent. The more images we have available, the more reliable the estimate can be. Examining the satellite images above we observe that not all of the images would be useful, as some of the lake area is covered with clouds. To overcome this limitation and have a denser time series we could rely on synthetic aperture radar observations, for example from the Copernicus Sentinel-1 platform.\n\n","type":"content","url":"/notebooks/inland-water-with-edc#how-to-visualise-your-datacube","position":11},{"hierarchy":{"lvl1":"Exploring Inland Water Bodies with Sentinel-2 data in EDC","lvl3":"Sentinel-1 description","lvl2":"Configuration"},"type":"lvl3","url":"/notebooks/inland-water-with-edc#sentinel-1-description","position":12},{"hierarchy":{"lvl1":"Exploring Inland Water Bodies with Sentinel-2 data in EDC","lvl3":"Sentinel-1 description","lvl2":"Configuration"},"content":"Like Sentinel-2, Sentinel-1 is also part of the Copernicus programme and collects data globally with a revisit time of 5 days. In contrast to Sentinel-2, Sentinel-1 SAR is an active sensor using SAR signals recording the backscatter. Due to the wavelengths used, SAR is not hindered by clouds and can be operated day and night.\n\n","type":"content","url":"/notebooks/inland-water-with-edc#sentinel-1-description","position":13},{"hierarchy":{"lvl1":"Exploring Inland Water Bodies with Sentinel-2 data in EDC","lvl3":"Check Sentinel-1 GRD available bands","lvl2":"Configuration"},"type":"lvl3","url":"/notebooks/inland-water-with-edc#check-sentinel-1-grd-available-bands","position":14},{"hierarchy":{"lvl1":"Exploring Inland Water Bodies with Sentinel-2 data in EDC","lvl3":"Check Sentinel-1 GRD available bands","lvl2":"Configuration"},"content":"Using EDC inbuilt functions that query Sentinel Hub services, we can easily list the available bands for a given dataset.\n\n# List bands for S1-GRD\nSH.band_names('S1GRD')\n\n","type":"content","url":"/notebooks/inland-water-with-edc#check-sentinel-1-grd-available-bands","position":15},{"hierarchy":{"lvl1":"Exploring Inland Water Bodies with Sentinel-2 data in EDC","lvl3":"Build an xcube","lvl2":"Configuration"},"type":"lvl3","url":"/notebooks/inland-water-with-edc#build-an-xcube","position":16},{"hierarchy":{"lvl1":"Exploring Inland Water Bodies with Sentinel-2 data in EDC","lvl3":"Build an xcube","lvl2":"Configuration"},"content":"In the following cell we will specify the input parameters needed to build an xcube array. The following parameters are specified:\n\ndataset_name: the Sentinel Hub identification of the dataset. Here we will call S1GRD for Sentinel-1 GRD. All available datasets can be listed with SH.dataset_names\n\nband_names: the band names to be used in the xcube array (see previous code cell). Here, we will call just the VV polarisation band.\n\nbbox: the bounding box that sets the extent of the AOI. Because we are using the default WGS84 coordinate system here, the CRS parameter doesn’t need to be set.\n\nspatial_res: the spatial resolution of the rasters contained in the xcube array. The spatial resolution is expressed in the units of the coordinate system used. Therefore, in this example, the spatial resolution is set in degrees. For an approximate pixel size of 10 meters, we set the resolution to 0.000089 degrees.\n\ntime_range: a list of two dates [start_date, end_date] forming a time period for which all acquisitions will be returned. Sentinel-1 GRD data is available from February 2015 onwards. In this example, we will fetch data for June 2023 - mid July 2023.\n\ntime_tolerance: The tolerance used to identify whether a dataset should still be included within a time period. Here, 30m corresponds to 30 minutes, thus avoiding duplicate datasets.\n\n# Setup xcube\ns1_cube_config = CubeConfig(dataset_name='S1GRD',\n                         band_names=['VV'],\n                         bbox=lake_bbox,\n                         spatial_res=0.000089,\n                         time_range=['2023-06-01', '2023-07-16'],\n                         time_tolerance='30m')\n\n","type":"content","url":"/notebooks/inland-water-with-edc#build-an-xcube","position":17},{"hierarchy":{"lvl1":"Exploring Inland Water Bodies with Sentinel-2 data in EDC","lvl3":"Open the xcube","lvl2":"Configuration"},"type":"lvl3","url":"/notebooks/inland-water-with-edc#open-the-xcube","position":18},{"hierarchy":{"lvl1":"Exploring Inland Water Bodies with Sentinel-2 data in EDC","lvl3":"Open the xcube","lvl2":"Configuration"},"content":"In the following cell we open the cube and display its contents. The automatically generated credentials obtained earlier in this Jupyter Notebook are specified as a parameter when opening the cube.\n\n# Open cube (on the fly)\ns1_cube = open_cube(s1_cube_config, **sh_credentials)\n\n# Display contents\ns1_cube\n\n","type":"content","url":"/notebooks/inland-water-with-edc#open-the-xcube","position":19},{"hierarchy":{"lvl1":"Exploring Inland Water Bodies with Sentinel-2 data in EDC","lvl3":"Visualising the water areas using SAR","lvl2":"Configuration"},"type":"lvl3","url":"/notebooks/inland-water-with-edc#visualising-the-water-areas-using-sar","position":20},{"hierarchy":{"lvl1":"Exploring Inland Water Bodies with Sentinel-2 data in EDC","lvl3":"Visualising the water areas using SAR","lvl2":"Configuration"},"content":"We are going to use the VV band to visualise the flooding. From our earlier visualisation, we know that the area had some cloud coverage around the date 2023-07-04. We will search for Sentinel-1 acquisitions around that date in order to obtain a denser time series.\n\nRadar data has very large dynamic range a very imbalanced histogram (>95% of all values are smaller than 1, but the remaining 5 % can be impractically large). Thus, to obtain a visualisation with a better contrast in which the water bodies would be darker and the land pixels would be brighter, it is recommended to convert the data to log-scale.\nTo convert the pixel values from Digital Number to decibels we can mutiply the log10 of each DN pixel by 10. Secondly, as there will be pixels with a value of -inf after this operation, we need to account for this with the second function which will automatically assign 0 to these pixels.\n\n# Convert VV Digital numbers to Decibels\nvv_dn = s1_cube.VV\nvv_db = 10 * (np.log10(vv_dn))\n\nvv_db = vv_db.where(np.isfinite(vv_db), 0)\n\nvv_db.attrs['long_name']='VV_dB'\nvv_db.attrs['units']='decibels'\n\ns1_cube['VV_dB']= vv_db\n\nLike previously, we are going to visualise the VV_dB variable we have just generated for our AOI.\n\n# select and define the timestamp you want to visualise \nVV_dB_timestamp = s1_cube.VV_dB.sel(time='2023-07-09 10:00:00', method='nearest')\n\n# plot the timestamp\nVV_dB_timestamp.plot.imshow(vmin=-40, vmax=0, cmap='winter', figsize=(10, 10))\n\n# save and display the plot\nplt.show()\n\nThis looks very similar to the NDWI we derived earlier showing the water extent fairly clearly (the blue areas). Let’s visualise it over several timestamps to confirm that this is a good variable to use to generate a lake mask.\n\n#### Timestamp selection\nvv1 = s1_cube.VV_dB.sel(time='2023-06-03 10:00:00', method='nearest')\nvv2 = s1_cube.VV_dB.sel(time='2023-06-04 10:00:00', method='nearest')\nvv3 = s1_cube.VV_dB.sel(time='2023-06-15 10:00:00', method='nearest')\nvv4 = s1_cube.VV_dB.sel(time='2023-06-27 10:00:00', method='nearest')\nvv5 = s1_cube.VV_dB.sel(time='2023-07-09 10:00:00', method='nearest')\nvv6 = s1_cube.VV_dB.sel(time='2023-07-10 10:00:00', method='nearest')\n\n# Plot \nf = plt.figure(figsize=[16,11])\nax1 = f.add_subplot(2,3, 1)\nax2 = f.add_subplot(2,3, 2)\nax3 = f.add_subplot(2,3, 3)\nax4 = f.add_subplot(2,3, 4)\nax5 = f.add_subplot(2,3, 5)\nax6 = f.add_subplot(2,3, 6)\n\naxlist=[ax1,ax2,ax3,ax4,ax5,ax6]\n\nt = vv1.plot.imshow(ax=ax1, vmin=-40, vmax=0, cmap='winter', add_colorbar=False)\nvv2.plot.imshow(ax=ax2, vmin=-40, vmax=0, cmap='winter', add_colorbar=False)\nvv3.plot.imshow(ax=ax3, vmin=-40, vmax=0, cmap='winter', add_colorbar=False)\nvv4.plot.imshow(ax=ax4, vmin=-40, vmax=0, cmap='winter', add_colorbar=False)\nvv5.plot.imshow(ax=ax5, vmin=-40, vmax=0, cmap='winter', add_colorbar=False)\nvv6.plot.imshow(ax=ax6, vmin=-40, vmax=0, cmap='winter', add_colorbar=False)\n\ncbar_ax = f.add_axes([1, 0.15, 0.05, 0.7])\nf.colorbar(t, cax=cbar_ax, label=\"VV dB\")\n\nplt.show()\n\nDepending on the local conditions (e.g. terrain orientation, slope) and weather conditions, optical or radar imagery may be more useful. In this case both types of observations seem to provide a good view of the water surface area extent.\n\nNext we will generate a flood mask using a threshold.\n\nfor SAR images, generally, as a good rule of thumb, in the VV band, values below -20 dB are usually surface water. We will try this value first, but we will also look to visualise how the flood mask changes if we adjust the threshold value.\n\nobserving the NDWI range of values, we can chose values above 0.25 to correspond to the water class.\n\nFirst, let’s generate the new variable using the .where function in xarray.\n\nAt first glance, the below cell may not make much sense. It may read that the step 1 function as assigning a value of 1 to pixels in VV_dB that are equal or more than -20. However, what is actually happening is that the .where function preserves all the pixel values in the variable that are below -20 and assigns everything else a value of 1. More can be found in the xarray documentation \n\nhttp://​xarray​.pydata​.org​/en​/stable​/generated​/xarray​.DataArray​.where​.html\n\n# mask the Sentinel-1 data\n\n# Assign all pixels equal or smaller than -20 a value of 1 and preserve the values of all other pixels \nstep1 = s1_cube.VV_dB.where(s1_cube.VV_dB > -20, 1)\n\n# Assign all other pixels a value of 0. \nwater = step1.where(step1 == 1, 0)\n\nwater.attrs['long_name'] ='water'\nwater.attrs['units'] ='nounits'\n\ns1_cube['water'] = water\n\nNext let’s see what happens to the mask extent if we change the threshold to -15 dB and -25dB:\n\n# Sentinel-1\nwater_threshold1_step1 = s1_cube.VV_dB.where(s1_cube.VV_dB > -15, 1)\nwater_threshold2_step1 = s1_cube.VV_dB.where(s1_cube.VV_dB > -20, 1)\nwater_threshold3_step1 = s1_cube.VV_dB.where(s1_cube.VV_dB > -25, 1)\n\nwater_threshold1_step2 = water_threshold1_step1.where(water_threshold1_step1 == 1, 0)\nwater_threshold2_step2 = water_threshold2_step1.where(water_threshold2_step1 == 1, 0)\nwater_threshold3_step2 = water_threshold3_step1.where(water_threshold3_step1 == 1, 0)\n\nwater_threshold1 = water_threshold1_step2.sel(time='2023-06-15 10:00:00', method='nearest')\nwater_threshold2 = water_threshold2_step2.sel(time='2023-06-15 10:00:00', method='nearest')\nwater_threshold3 = water_threshold3_step2.sel(time='2023-06-15 10:00:00', method='nearest')\n\nNext we will plot the new thresholds we want to test:\n\n# Plot \nf = plt.figure(figsize=[20,12])\nax1 = f.add_subplot(2,3, 1)\nax2 = f.add_subplot(2,3, 2)\nax3 = f.add_subplot(2,3, 3)\n\nwater_threshold1.plot.imshow(ax=ax1, vmin=0, vmax=1, cmap='Blues', add_colorbar=False)\nwater_threshold2.plot.imshow(ax=ax2, vmin=0, vmax=1, cmap='Blues', add_colorbar=False)\nwater_threshold3.plot.imshow(ax=ax3, vmin=0, vmax=1, cmap='Blues', add_colorbar=False)\n\nplt.show()\n\n# mask the Sentinel-2 data\n\n# Assign all pixels equal or larger than 0.25 a value of 1 and preserve the values of pixels \nstep11 = ndwi1.where(ndwi1 < 0.25, 1)\n\n# Assign all other pixels a value of 0. \nwater1 = step1.where(step1 == 1, 0)\n\nwater1.attrs['long_name'] ='water'\nwater1.attrs['units'] ='nounits'\n\ns2_cube['water'] = water1\n\n# Sentinel-2\nwater_threshold1_step11 = ndwi1.where(ndwi1 < 0.15, 1)\nwater_threshold2_step11 = ndwi1.where(ndwi1 < 0.25, 1)\nwater_threshold3_step11 = ndwi1.where(ndwi1 < 0.30, 1)\n\nwater_threshold1_step21 = water_threshold1_step11.where(water_threshold1_step11 == 1, 0)\nwater_threshold2_step21 = water_threshold2_step11.where(water_threshold2_step11 == 1, 0)\nwater_threshold3_step21 = water_threshold3_step11.where(water_threshold3_step11 == 1, 0)\n\nwater_threshold11 = water_threshold1_step21\nwater_threshold21 = water_threshold2_step21\nwater_threshold31 = water_threshold3_step21\n\n# Plot \nf = plt.figure(figsize=[20,12])\nax1 = f.add_subplot(2,3, 1)\nax2 = f.add_subplot(2,3, 2)\nax3 = f.add_subplot(2,3, 3)\n\nwater_threshold11.plot.imshow(ax=ax1, vmin=0, vmax=1, cmap='Blues', add_colorbar=False)\nwater_threshold21.plot.imshow(ax=ax2, vmin=0, vmax=1, cmap='Blues', add_colorbar=False)\nwater_threshold31.plot.imshow(ax=ax3, vmin=0, vmax=1, cmap='Blues', add_colorbar=False)\n\nplt.show()\n\nObserve the differences in the water mask due to the threshold.\n\n","type":"content","url":"/notebooks/inland-water-with-edc#visualising-the-water-areas-using-sar","position":21},{"hierarchy":{"lvl1":"Exploring Inland Water Bodies with Sentinel-2 data in EDC","lvl3":"Estimating the surface water area extent","lvl2":"Configuration"},"type":"lvl3","url":"/notebooks/inland-water-with-edc#estimating-the-surface-water-area-extent","position":22},{"hierarchy":{"lvl1":"Exploring Inland Water Bodies with Sentinel-2 data in EDC","lvl3":"Estimating the surface water area extent","lvl2":"Configuration"},"content":"Let’s estimate the area covered by water during the time period we are examining.\n\nWe can also estimate which is the area that is most covered by water by dividing the sum of the water pixels by the number of timesteps in the data cube (the count).\n\n# previously we only kept 1 time step when we selected ndwi1\n# now we want to keep the full datacube in order to average in time\n\n# Assign all pixels equal or larger than 0.25 a value of 1 and preserve the values of all other pixels \nstep1 = s2_cube.NDWI.where(s2_cube.NDWI < 0.25, 1)\n\n# Assign all other pixels a value of 0. \nwater = step1.where(step1 == 1, 0)\n\nwater.attrs['long_name'] ='water'\nwater.attrs['units'] ='nounits'\n\ns2_cube['water'] = water\n\n\nwater_sum = s2_cube.NDWI.sum(dim=\"time\")\nwater_count = s2_cube.NDWI.count(dim=\"time\")\nwater_average = water_sum / water_count\n\n\nwater_average.attrs['long_name']='water area'\nwater_average.attrs['units']='nounits'\n\nndwi['water_average']= water_average\n\nNow let’s plot the water_average into a plot:\n\nwater_average.plot.imshow(cmap='GnBu', vmin=0, vmax=0.5, figsize=(10, 10))\n\nplt.tight_layout()\n\n#expport to png\nplt.savefig('figure.png')\n\nplt.show()\n\nThis looks great, we have identified the lake area very clearly here and can also observe how the lake may change in size over time.\n\n","type":"content","url":"/notebooks/inland-water-with-edc#estimating-the-surface-water-area-extent","position":23},{"hierarchy":{"lvl1":"Exploring Inland Water Bodies with Sentinel-2 data in EDC","lvl2":"Access environmental variables and other datasets"},"type":"lvl2","url":"/notebooks/inland-water-with-edc#access-environmental-variables-and-other-datasets","position":24},{"hierarchy":{"lvl1":"Exploring Inland Water Bodies with Sentinel-2 data in EDC","lvl2":"Access environmental variables and other datasets"},"content":"This \n\nNotebook demonstrates how to:\n\nrequest data from selected Copernicus Services,\nrequest data from Copernicus Climate Data Store,\nrequest data from ESA Climate Change Initiative.","type":"content","url":"/notebooks/inland-water-with-edc#access-environmental-variables-and-other-datasets","position":25},{"hierarchy":{"lvl1":"Earth System Science Open Challenge 1: L. Jessel, S. Hezzi, M. Roussel"},"type":"lvl1","url":"/notebooks/openchallengenotebook-c1-jessel-hezzi-roussel","position":0},{"hierarchy":{"lvl1":"Earth System Science Open Challenge 1: L. Jessel, S. Hezzi, M. Roussel"},"content":"\n\nThis concise notebook demonstrates guidelines for submission of your projects developed under the Earth System Science Open Challenge! Please use this notebook as a template for delivering your workflow with the code you used to produce the results! For more information, refer to Open Challenge website: \n\nhttps://​eo4society​.esa​.int​/event​/sciencehubchallengefeb2024/\n\n","type":"content","url":"/notebooks/openchallengenotebook-c1-jessel-hezzi-roussel","position":1},{"hierarchy":{"lvl1":"Earth System Science Open Challenge 1: L. Jessel, S. Hezzi, M. Roussel"},"type":"lvl1","url":"/notebooks/openchallengenotebook-c1-jessel-hezzi-roussel#earth-system-science-open-challenge-1-l-jessel-s-hezzi-m-roussel","position":2},{"hierarchy":{"lvl1":"Earth System Science Open Challenge 1: L. Jessel, S. Hezzi, M. Roussel"},"content":"\n\n","type":"content","url":"/notebooks/openchallengenotebook-c1-jessel-hezzi-roussel#earth-system-science-open-challenge-1-l-jessel-s-hezzi-m-roussel","position":3},{"hierarchy":{"lvl1":"1. Title "},"type":"lvl1","url":"/notebooks/openchallengenotebook-c1-jessel-hezzi-roussel#id-1-title","position":4},{"hierarchy":{"lvl1":"1. Title "},"content":"Author(s): Lucas Jessel, Sabrine Hezzi, Marie-Laure Roussel  \nGroup name: Extreme precip  \nChallenge: 1 - “Cevenols episodes” or rainfall extreme events in the South-East of France \n\nSubmission date: 01/03/2024 \n\n\n","type":"content","url":"/notebooks/openchallengenotebook-c1-jessel-hezzi-roussel#id-1-title","position":5},{"hierarchy":{"lvl1":"2. Description "},"type":"lvl1","url":"/notebooks/openchallengenotebook-c1-jessel-hezzi-roussel#id-2-description","position":6},{"hierarchy":{"lvl1":"2. Description "},"content":"","type":"content","url":"/notebooks/openchallengenotebook-c1-jessel-hezzi-roussel#id-2-description","position":7},{"hierarchy":{"lvl1":"2. Description ","lvl3":"Description of research approach"},"type":"lvl3","url":"/notebooks/openchallengenotebook-c1-jessel-hezzi-roussel#description-of-research-approach","position":8},{"hierarchy":{"lvl1":"2. Description ","lvl3":"Description of research approach"},"content":"Mediterranean events of intense rainfall can happen in late summer or in fall in the south-east of France (Cevennes, Rhone valley, Roussillon, Provence) due to warm and humid air coming from the heated sea, thanks to south-east flux mainly driven by any disturbance on France. It causes thunderstorms and heavy precipitation because of the cold air in altitude (that is explained in this region because of the topography : Massif Central, Alpes, Pyrenees). Flooding, runoff and landslides are the most important consequences of these events, due to blocked rainfall in the area.\n\nMonitoring this kind of extreme event is a massive challenge for weather forecasts models, that is why observations from space are a really great tool to help in that way.\n\nIn this study, the objective is to use observations products from space to try to detect massive rainfall events and evaluate the accuracy of the results in terms of caracteristics of the precipitation event (daily maximum value and location).\n\n","type":"content","url":"/notebooks/openchallengenotebook-c1-jessel-hezzi-roussel#description-of-research-approach","position":9},{"hierarchy":{"lvl1":"3. Table of Contents "},"type":"lvl1","url":"/notebooks/openchallengenotebook-c1-jessel-hezzi-roussel#id-3-table-of-contents","position":10},{"hierarchy":{"lvl1":"3. Table of Contents "},"content":"Title\n\nDescription\n\nTable of Contents\n\nReferences\n\nKey Conclusions\n\nSocietal Context\n\nImport libraries\n\nAccess dataset\n\nAnalysis cells\n\n","type":"content","url":"/notebooks/openchallengenotebook-c1-jessel-hezzi-roussel#id-3-table-of-contents","position":11},{"hierarchy":{"lvl1":"4. References "},"type":"lvl1","url":"/notebooks/openchallengenotebook-c1-jessel-hezzi-roussel#id-4-references","position":12},{"hierarchy":{"lvl1":"4. References "},"content":"Brocca et al. 2014. Soil as a natural rain gauge: Estimating global rainfall from satellite soil moisture data, \n\nBrocca et al. (2014).\n\nHuffman et al. 2001. Global Precipitation at One-Degree Daily Resolution from Multisatellite Observations, \n\nhttps://​doi​.org​/10​.1175​/1525​-7541(2001)002<0036:GPAODD>\n\n2.0.CO;2\n\n","type":"content","url":"/notebooks/openchallengenotebook-c1-jessel-hezzi-roussel#id-4-references","position":13},{"hierarchy":{"lvl1":"5. Key Conclusions "},"type":"lvl1","url":"/notebooks/openchallengenotebook-c1-jessel-hezzi-roussel#id-5-key-conclusions","position":14},{"hierarchy":{"lvl1":"5. Key Conclusions "},"content":"Among the two observational datasets compared to ERA5 reanalyses, which we have taken as a reference, it emerges as a conclusion that the GPM-CPC product provides a more accurate and faithful representation of an extreme Cevenol precipitation event, such as the one studied, both for the location and the value of the maximum of precipitation.\n\nOn the contrary, the GPCP product appears to produce too little precipitation for a Mediterranean episode day and fails to reproduce the maximum precipitation at the correct location.\n\nA more in-depth analysis could be considered to account for differences in spatial resolution between the datasets, as well as to study different days over a longer period of time.\n\n","type":"content","url":"/notebooks/openchallengenotebook-c1-jessel-hezzi-roussel#id-5-key-conclusions","position":15},{"hierarchy":{"lvl1":"6. Societal Context "},"type":"lvl1","url":"/notebooks/openchallengenotebook-c1-jessel-hezzi-roussel#id-6-societal-context","position":16},{"hierarchy":{"lvl1":"6. Societal Context "},"content":"Mediterranean episodes can be dangerous for the population due to their significant and difficult-to-predict consequences such as floods and landslides. Numerous damages need to be considered during these sometimes dramatic events.\n\n","type":"content","url":"/notebooks/openchallengenotebook-c1-jessel-hezzi-roussel#id-6-societal-context","position":17},{"hierarchy":{"lvl1":"PART 2: SCIENTIFIC EXPLOITATION AND ANALYSIS"},"type":"lvl1","url":"/notebooks/openchallengenotebook-c1-jessel-hezzi-roussel#part-2-scientific-exploitation-and-analysis","position":18},{"hierarchy":{"lvl1":"PART 2: SCIENTIFIC EXPLOITATION AND ANALYSIS"},"content":"\n\n","type":"content","url":"/notebooks/openchallengenotebook-c1-jessel-hezzi-roussel#part-2-scientific-exploitation-and-analysis","position":19},{"hierarchy":{"lvl1":"7. Import Libraries "},"type":"lvl1","url":"/notebooks/openchallengenotebook-c1-jessel-hezzi-roussel#id-7-import-libraries","position":20},{"hierarchy":{"lvl1":"7. Import Libraries "},"content":"This notebook runs with the python environment deepesdl-xcube-1.1.2, please checkout the documentation for \n\nhelp on changing the environment.\n\nimport xcube\nfrom xcube.core.store import find_data_store_extensions\nfrom xcube.core.store import get_data_store_params_schema\nfrom xcube.core.store import new_data_store\n\nimport boto3\nfrom botocore.exceptions import NoCredentialsError\n\nimport os\nimport shapely.geometry\nfrom IPython.display import JSON\nimport numpy as np\nimport xarray as xr\n\n# for the plots\nfrom cartopy import crs as ccrs, feature as cfeature\nprojPC = ccrs.PlateCarree(central_longitude=0)\nimport matplotlib.pyplot as plt\nimport matplotlib.colors as mcolors\nimport matplotlib.ticker as ticker\n%matplotlib inline\nplt.rcParams[\"figure.figsize\"] = 16,8\n\n","type":"content","url":"/notebooks/openchallengenotebook-c1-jessel-hezzi-roussel#id-7-import-libraries","position":21},{"hierarchy":{"lvl1":"8. Data sources "},"type":"lvl1","url":"/notebooks/openchallengenotebook-c1-jessel-hezzi-roussel#id-8-data-sources","position":22},{"hierarchy":{"lvl1":"8. Data sources "},"content":"\n\nDatacube name\n\nVariable name\n\nDescription\n\nReference*\n\nRegion\n\nTime range\n\nResolution\n\n(GPM-CPC) hydrology-1D-0.009deg-100x60x60-3.0.2.zarr\n\nprecip\n\nmetadata description\n\nlink to source\n\nsouth-east of France (40-45°N, 0-10°E)\n\ndaily\n\n0.01°\n\n(GPCP) cube_GPCP_complete_nomissingvalue_lat_1996-2022.zarr\n\nprecip\n\nmetadata description\n\nlink to source\n\nsouth-east of France (40-45°N, 0-10°E)\n\ndaily\n\n1°\n\n(ERA5) cube_tpsum.2015-2022.fs1e5.GLOBAL_025.zarr\n\ntp\n\nmetadata description\n\nlink to source\n\nsouth-east of France (40-45°N, 0-10°E)\n\ndaily\n\n0.25°\n\n# CODE SECTION\n\n","type":"content","url":"/notebooks/openchallengenotebook-c1-jessel-hezzi-roussel#id-8-data-sources","position":23},{"hierarchy":{"lvl1":"8. Data sources ","lvl4":"Setting stores variables"},"type":"lvl4","url":"/notebooks/openchallengenotebook-c1-jessel-hezzi-roussel#setting-stores-variables","position":24},{"hierarchy":{"lvl1":"8. Data sources ","lvl4":"Setting stores variables"},"content":"\n\n# PUBLIC STORE : for GPM-CPC dataset in the hydrology cube\npublic_store = new_data_store(\"s3\", root=\"deep-esdl-public\", storage_options=dict(anon=True))\n\n# USER/TEAM STORE : for GPCP and ERA5 datasets in the \"homemade\" cubes\nS3_USER_STORAGE_KEY = os.environ[\"S3_USER_STORAGE_KEY\"]\nS3_USER_STORAGE_SECRET = os.environ[\"S3_USER_STORAGE_SECRET\"]\nS3_USER_STORAGE_BUCKET = os.environ[\"endpoint\"]\nuser_store = new_data_store(\"s3\", max_depth=3, root=S3_USER_STORAGE_BUCKET,storage_options=dict(anon=False, key=S3_USER_STORAGE_KEY,secret=S3_USER_STORAGE_SECRET))\n\n","type":"content","url":"/notebooks/openchallengenotebook-c1-jessel-hezzi-roussel#setting-stores-variables","position":25},{"hierarchy":{"lvl1":"8. Data sources ","lvl4":"Extracting GPM-CPC SM2RAIN ASCAT Dataset (from hydrology cube)"},"type":"lvl4","url":"/notebooks/openchallengenotebook-c1-jessel-hezzi-roussel#extracting-gpm-cpc-sm2rain-ascat-dataset-from-hydrology-cube","position":26},{"hierarchy":{"lvl1":"8. Data sources ","lvl4":"Extracting GPM-CPC SM2RAIN ASCAT Dataset (from hydrology cube)"},"content":"\n\nhydro_cube = public_store.open_data('hydrology-1D-0.009deg-100x60x60-3.0.2.zarr')\nprecip_data_hydro=hydro_cube['precip']\nprecip_data_hydro\n\n","type":"content","url":"/notebooks/openchallengenotebook-c1-jessel-hezzi-roussel#extracting-gpm-cpc-sm2rain-ascat-dataset-from-hydrology-cube","position":27},{"hierarchy":{"lvl1":"8. Data sources ","lvl4":"Extracting GPCP Dataset"},"type":"lvl4","url":"/notebooks/openchallengenotebook-c1-jessel-hezzi-roussel#extracting-gpcp-dataset","position":28},{"hierarchy":{"lvl1":"8. Data sources ","lvl4":"Extracting GPCP Dataset"},"content":"\n\ngpcp_cube = user_store.open_data('cube_GPCP_complete_nomissingvalue_lat_1996-2022.zarr')\nprecip_data_gpcp = gpcp_cube['precip']\nprecip_data_gpcp\n\n","type":"content","url":"/notebooks/openchallengenotebook-c1-jessel-hezzi-roussel#extracting-gpcp-dataset","position":29},{"hierarchy":{"lvl1":"8. Data sources ","lvl4":"Extracting ERA5 Dataset"},"type":"lvl4","url":"/notebooks/openchallengenotebook-c1-jessel-hezzi-roussel#extracting-era5-dataset","position":30},{"hierarchy":{"lvl1":"8. Data sources ","lvl4":"Extracting ERA5 Dataset"},"content":"\n\nera_cube = user_store.open_data('cube_tpsum.2015-2022.fs1e5.GLOBAL_025.zarr')\nprecip_data_era = era_cube['tp']*3600. # convert mm/s to mm/d \nprecip_data_era\n\n\n\n\n\n","type":"content","url":"/notebooks/openchallengenotebook-c1-jessel-hezzi-roussel#extracting-era5-dataset","position":31},{"hierarchy":{"lvl1":"9. Analysis cells "},"type":"lvl1","url":"/notebooks/openchallengenotebook-c1-jessel-hezzi-roussel#id-9-analysis-cells","position":32},{"hierarchy":{"lvl1":"9. Analysis cells "},"content":"\n\n# defining some coordinates inside the region of interest\n\nlat1=45 ; lon1=5\nlat2=45 ; lon2=7\nlat3=43 ; lon3=3\nlat4=44 ; lon4=7\nlat5=44 ; lon5=4\nlat6=44 ; lon6=5\nlat7=44 ; lon7=6\nlat8=44 ; lon8=3\n\n","type":"content","url":"/notebooks/openchallengenotebook-c1-jessel-hezzi-roussel#id-9-analysis-cells","position":33},{"hierarchy":{"lvl1":"9. Analysis cells ","lvl3":"9.1) Plots of the long-term time-series"},"type":"lvl3","url":"/notebooks/openchallengenotebook-c1-jessel-hezzi-roussel#id-9-1-plots-of-the-long-term-time-series","position":34},{"hierarchy":{"lvl1":"9. Analysis cells ","lvl3":"9.1) Plots of the long-term time-series"},"content":"\n\n","type":"content","url":"/notebooks/openchallengenotebook-c1-jessel-hezzi-roussel#id-9-1-plots-of-the-long-term-time-series","position":35},{"hierarchy":{"lvl1":"9. Analysis cells ","lvl4":"9.1.1) From the hydrology cube :","lvl3":"9.1) Plots of the long-term time-series"},"type":"lvl4","url":"/notebooks/openchallengenotebook-c1-jessel-hezzi-roussel#id-9-1-1-from-the-hydrology-cube","position":36},{"hierarchy":{"lvl1":"9. Analysis cells ","lvl4":"9.1.1) From the hydrology cube :","lvl3":"9.1) Plots of the long-term time-series"},"content":"\n\n# selecting local precipitation data\n\nprecip_point1 = precip_data_hydro.sel(lat=lat1, lon=lon1, method='nearest')\nprecip_point2 = precip_data_hydro.sel(lat=lat2, lon=lon2, method='nearest')\nprecip_point3 = precip_data_hydro.sel(lat=lat3, lon=lon3, method='nearest')\nprecip_point4 = precip_data_hydro.sel(lat=lat4, lon=lon4, method='nearest')\nprecip_point5 = precip_data_hydro.sel(lat=lat5, lon=lon5, method='nearest')\nprecip_point6 = precip_data_hydro.sel(lat=lat6, lon=lon6, method='nearest')\nprecip_point7 = precip_data_hydro.sel(lat=lat7, lon=lon7, method='nearest')\nprecip_point8 = precip_data_hydro.sel(lat=lat8, lon=lon8, method='nearest')\n\nplt.figure(figsize=(20, 6))\nprecip_point1.plot(marker='o', linestyle='-',color='blue')\nprecip_point2.plot(marker='o', linestyle='-',color='red')\nprecip_point3.plot(marker='o', linestyle='-',color='green')\nprecip_point4.plot(marker='o', linestyle='-',color='yellow')\nprecip_point5.plot(marker='o', linestyle='-',color='purple')\nprecip_point6.plot(marker='o', linestyle='-',color='cyan')\nprecip_point7.plot(marker='o', linestyle='-',color='magenta')\nprecip_point8.plot(marker='o', linestyle='-',color='orange')\nplt.xlabel('Day')\nplt.ylabel('Daily rainfall (mm/d)')\nplt.title('Precipitation from GPM-CPC SM2RAIN-ASCAT (hydrology cube)')\nplt.grid(True)\nplt.show()\n\n","type":"content","url":"/notebooks/openchallengenotebook-c1-jessel-hezzi-roussel#id-9-1-1-from-the-hydrology-cube","position":37},{"hierarchy":{"lvl1":"9. Analysis cells ","lvl4":"9.1.2) From the GPCP cube :","lvl3":"9.1) Plots of the long-term time-series"},"type":"lvl4","url":"/notebooks/openchallengenotebook-c1-jessel-hezzi-roussel#id-9-1-2-from-the-gpcp-cube","position":38},{"hierarchy":{"lvl1":"9. Analysis cells ","lvl4":"9.1.2) From the GPCP cube :","lvl3":"9.1) Plots of the long-term time-series"},"content":"\n\n# selecting local precipitation data\nprecip_point1 = precip_data_gpcp.sel(latitude=lat1, longitude=lon1, method='nearest')\nprecip_point2 = precip_data_gpcp.sel(latitude=lat2, longitude=lon2, method='nearest')\nprecip_point3 = precip_data_gpcp.sel(latitude=lat3, longitude=lon3, method='nearest')\nprecip_point4 = precip_data_gpcp.sel(latitude=lat4, longitude=lon4, method='nearest')\nprecip_point5 = precip_data_gpcp.sel(latitude=lat5, longitude=lon5, method='nearest')\nprecip_point6 = precip_data_gpcp.sel(latitude=lat6, longitude=lon6, method='nearest')\nprecip_point7 = precip_data_gpcp.sel(latitude=lat7, longitude=lon7, method='nearest')\nprecip_point8 = precip_data_gpcp.sel(latitude=lat8, longitude=lon8, method='nearest')\n\nplt.figure(figsize=(20, 6))\nprecip_point1.plot(marker='o', linestyle='-',color='blue')\nprecip_point2.plot(marker='o', linestyle='-',color='red')\nprecip_point3.plot(marker='o', linestyle='-',color='green')\nprecip_point4.plot(marker='o', linestyle='-',color='yellow')\nprecip_point5.plot(marker='o', linestyle='-',color='purple')\nprecip_point6.plot(marker='o', linestyle='-',color='cyan')\nprecip_point7.plot(marker='o', linestyle='-',color='magenta')\nprecip_point8.plot(marker='o', linestyle='-',color='orange')\n\nplt.xlabel('Day')\nplt.ylabel('Daily rainfall (mm/d)')\nplt.ylim(0,80)\nplt.title('Precipitation from GPCP')\nplt.grid(True)\nplt.show()\n\n","type":"content","url":"/notebooks/openchallengenotebook-c1-jessel-hezzi-roussel#id-9-1-2-from-the-gpcp-cube","position":39},{"hierarchy":{"lvl1":"9. Analysis cells ","lvl4":"9.1.3) From the ERA5 cube :","lvl3":"9.1) Plots of the long-term time-series"},"type":"lvl4","url":"/notebooks/openchallengenotebook-c1-jessel-hezzi-roussel#id-9-1-3-from-the-era5-cube","position":40},{"hierarchy":{"lvl1":"9. Analysis cells ","lvl4":"9.1.3) From the ERA5 cube :","lvl3":"9.1) Plots of the long-term time-series"},"content":"\n\n# selecting local precipitation data\n\nprecip_point1 = precip_data_era.sel(latitude=lat1, longitude=lon1, method='nearest')\nprecip_point2 = precip_data_era.sel(latitude=lat2, longitude=lon2, method='nearest')\nprecip_point3 = precip_data_era.sel(latitude=lat3, longitude=lon3, method='nearest')\nprecip_point4 = precip_data_era.sel(latitude=lat4, longitude=lon4, method='nearest')\nprecip_point5 = precip_data_era.sel(latitude=lat5, longitude=lon5, method='nearest')\nprecip_point6 = precip_data_era.sel(latitude=lat6, longitude=lon6, method='nearest')\nprecip_point7 = precip_data_era.sel(latitude=lat7, longitude=lon7, method='nearest')\nprecip_point8 = precip_data_era.sel(latitude=lat8, longitude=lon8, method='nearest')\n\nplt.figure(figsize=(20, 6))\nprecip_point1.plot(marker='o', linestyle='-',color='blue')\nprecip_point2.plot(marker='o', linestyle='-',color='red')\nprecip_point3.plot(marker='o', linestyle='-',color='green')\nprecip_point4.plot(marker='o', linestyle='-',color='yellow')\nprecip_point5.plot(marker='o', linestyle='-',color='purple')\nprecip_point6.plot(marker='o', linestyle='-',color='cyan')\nprecip_point7.plot(marker='o', linestyle='-',color='magenta')\nprecip_point8.plot(marker='o', linestyle='-',color='orange')\n\nplt.xlabel('Day')\nplt.ylabel('Daily rainfall (mm/d)')\nplt.title('Precipitation from ERA5')\nplt.grid(True)\nplt.show()\n\n","type":"content","url":"/notebooks/openchallengenotebook-c1-jessel-hezzi-roussel#id-9-1-3-from-the-era5-cube","position":41},{"hierarchy":{"lvl1":"9. Analysis cells ","lvl3":"9.2) Filtering => minicube selection on the studied region"},"type":"lvl3","url":"/notebooks/openchallengenotebook-c1-jessel-hezzi-roussel#id-9-2-filtering-minicube-selection-on-the-studied-region","position":42},{"hierarchy":{"lvl1":"9. Analysis cells ","lvl3":"9.2) Filtering => minicube selection on the studied region"},"content":"\n\n","type":"content","url":"/notebooks/openchallengenotebook-c1-jessel-hezzi-roussel#id-9-2-filtering-minicube-selection-on-the-studied-region","position":43},{"hierarchy":{"lvl1":"9. Analysis cells ","lvl4":"9.2.1) On the hydrology cube","lvl3":"9.2) Filtering => minicube selection on the studied region"},"type":"lvl4","url":"/notebooks/openchallengenotebook-c1-jessel-hezzi-roussel#id-9-2-1-on-the-hydrology-cube","position":44},{"hierarchy":{"lvl1":"9. Analysis cells ","lvl4":"9.2.1) On the hydrology cube","lvl3":"9.2) Filtering => minicube selection on the studied region"},"content":"\n\nlatm=43; latM=45; lonm=2.5; lonM=8  \n\nhydro_region_precip = hydro_cube['precip'].sel(lat=slice(latM,latm), lon = slice(lonm,lonM))\nhydro_region_precip\n\n","type":"content","url":"/notebooks/openchallengenotebook-c1-jessel-hezzi-roussel#id-9-2-1-on-the-hydrology-cube","position":45},{"hierarchy":{"lvl1":"9. Analysis cells ","lvl4":"9.2.2) On the GPCP cube","lvl3":"9.2) Filtering => minicube selection on the studied region"},"type":"lvl4","url":"/notebooks/openchallengenotebook-c1-jessel-hezzi-roussel#id-9-2-2-on-the-gpcp-cube","position":46},{"hierarchy":{"lvl1":"9. Analysis cells ","lvl4":"9.2.2) On the GPCP cube","lvl3":"9.2) Filtering => minicube selection on the studied region"},"content":"\n\ngpcp_region_precip = gpcp_cube['precip'].sel(latitude=slice(latm,latM), longitude= slice(lonm,lonM))\ngpcp_region_precip\n\n","type":"content","url":"/notebooks/openchallengenotebook-c1-jessel-hezzi-roussel#id-9-2-2-on-the-gpcp-cube","position":47},{"hierarchy":{"lvl1":"9. Analysis cells ","lvl4":"9.2.3) On the ERA5 cube","lvl3":"9.2) Filtering => minicube selection on the studied region"},"type":"lvl4","url":"/notebooks/openchallengenotebook-c1-jessel-hezzi-roussel#id-9-2-3-on-the-era5-cube","position":48},{"hierarchy":{"lvl1":"9. Analysis cells ","lvl4":"9.2.3) On the ERA5 cube","lvl3":"9.2) Filtering => minicube selection on the studied region"},"content":"\n\nera_region_precip = era_cube['tp'].sel(latitude=slice(latM,latm), longitude= slice(lonm,lonM))\nera_region_precip \n\n","type":"content","url":"/notebooks/openchallengenotebook-c1-jessel-hezzi-roussel#id-9-2-3-on-the-era5-cube","position":49},{"hierarchy":{"lvl1":"9. Analysis cells ","lvl3":"9.3) Searching the maximum values at each grid point for the whole time-serie"},"type":"lvl3","url":"/notebooks/openchallengenotebook-c1-jessel-hezzi-roussel#id-9-3-searching-the-maximum-values-at-each-grid-point-for-the-whole-time-serie","position":50},{"hierarchy":{"lvl1":"9. Analysis cells ","lvl3":"9.3) Searching the maximum values at each grid point for the whole time-serie"},"content":"\n\n","type":"content","url":"/notebooks/openchallengenotebook-c1-jessel-hezzi-roussel#id-9-3-searching-the-maximum-values-at-each-grid-point-for-the-whole-time-serie","position":51},{"hierarchy":{"lvl1":"9. Analysis cells ","lvl4":"9.3.1) compute the maximum values on the region during the whole time-serie for each grid cell","lvl3":"9.3) Searching the maximum values at each grid point for the whole time-serie"},"type":"lvl4","url":"/notebooks/openchallengenotebook-c1-jessel-hezzi-roussel#id-9-3-1-compute-the-maximum-values-on-the-region-during-the-whole-time-serie-for-each-grid-cell","position":52},{"hierarchy":{"lvl1":"9. Analysis cells ","lvl4":"9.3.1) compute the maximum values on the region during the whole time-serie for each grid cell","lvl3":"9.3) Searching the maximum values at each grid point for the whole time-serie"},"content":"\n\nhydro_region_precip_max = hydro_region_precip.max(dim = ['time'])\n\ngpcp_region_precip_max = gpcp_cube['precip'].max(dim = ['time'])\n\nera_region_precip_max = era_region_precip.max(dim = ['time'])\n\n","type":"content","url":"/notebooks/openchallengenotebook-c1-jessel-hezzi-roussel#id-9-3-1-compute-the-maximum-values-on-the-region-during-the-whole-time-serie-for-each-grid-cell","position":53},{"hierarchy":{"lvl1":"9. Analysis cells ","lvl4":"9.3.2) Searching the days of massive precip events","lvl3":"9.3) Searching the maximum values at each grid point for the whole time-serie"},"type":"lvl4","url":"/notebooks/openchallengenotebook-c1-jessel-hezzi-roussel#id-9-3-2-searching-the-days-of-massive-precip-events","position":54},{"hierarchy":{"lvl1":"9. Analysis cells ","lvl4":"9.3.2) Searching the days of massive precip events","lvl3":"9.3) Searching the maximum values at each grid point for the whole time-serie"},"content":"\n\n# defining the threshold matrix (maximum values to consider extreme events) \n# applying to datasets to find dates (where the daily precip > threshold)\n\nhydro_threshold = 0.99 * hydro_region_precip_max  \nhydro_dates_sup_threshold = hydro_region_precip['time'].where(hydro_region_precip >= hydro_threshold)\nhydro_tt = hydro_dates_sup_threshold.dropna(dim='time', how='all')\n\ngpcp_threshold = 0.99 * gpcp_region_precip_max  \ngpcp_dates_sup_threshold = gpcp_region_precip['time'].where(gpcp_cube['precip'] >= gpcp_threshold)\ngpcp_tt = gpcp_dates_sup_threshold.dropna(dim='time', how='all')\n\nera_threshold = 0.99 * era_region_precip_max \nera_dates_sup_threshold = era_region_precip['time'].where(era_cube['tp'] >= era_threshold)\nera_tt = era_dates_sup_threshold.dropna(dim='time', how='all')\n\n","type":"content","url":"/notebooks/openchallengenotebook-c1-jessel-hezzi-roussel#id-9-3-2-searching-the-days-of-massive-precip-events","position":55},{"hierarchy":{"lvl1":"9. Analysis cells ","lvl2":">>> We select the 2nd of october 2020 (detected by GPM-CPC and GPCP)"},"type":"lvl2","url":"/notebooks/openchallengenotebook-c1-jessel-hezzi-roussel#id-we-select-the-2nd-of-october-2020-detected-by-gpm-cpc-and-gpcp","position":56},{"hierarchy":{"lvl1":"9. Analysis cells ","lvl2":">>> We select the 2nd of october 2020 (detected by GPM-CPC and GPCP)"},"content":"\n\n","type":"content","url":"/notebooks/openchallengenotebook-c1-jessel-hezzi-roussel#id-we-select-the-2nd-of-october-2020-detected-by-gpm-cpc-and-gpcp","position":57},{"hierarchy":{"lvl1":"9. Analysis cells ","lvl3":"9.4) Maps of the daily precipitation for the three datasets on that particular day","lvl2":">>> We select the 2nd of october 2020 (detected by GPM-CPC and GPCP)"},"type":"lvl3","url":"/notebooks/openchallengenotebook-c1-jessel-hezzi-roussel#id-9-4-maps-of-the-daily-precipitation-for-the-three-datasets-on-that-particular-day","position":58},{"hierarchy":{"lvl1":"9. Analysis cells ","lvl3":"9.4) Maps of the daily precipitation for the three datasets on that particular day","lvl2":">>> We select the 2nd of october 2020 (detected by GPM-CPC and GPCP)"},"content":"\n\nhydro_xtrem_precip_20201002 = hydro_cube['precip'].sel(time='2020-10-02') \n\nfig = plt.figure()\nax = plt.axes(projection = projPC)\nim = ax.pcolormesh(hydro_xtrem_precip_20201002.lon, hydro_xtrem_precip_20201002.lat, hydro_xtrem_precip_20201002.values)\nax.set_extent([lonm, lonM, latm, latM], crs=projPC)\ngl = ax.gridlines(draw_labels=True, linewidth=0, alpha=0.5)\n\nax.coastlines()          \nax.add_feature(cfeature.LAND)\nax.add_feature(cfeature.COASTLINE)\nax.add_feature(cfeature.LAKES, alpha=0.5)\nax.add_feature(cfeature.RIVERS)\ncbar = plt.colorbar(im, label='precipitation (mm/d)',  extend='max', anchor=(0.2, 0.5), shrink=0.6, aspect=20)  # ajout de la colorbar\nax.set_title('Total precipitation (mm) on the south-east of France on the 02/10/2020 - from GPM-CPC dataset') \n\ngpcp_xtrem_precip_20201002 = gpcp_region_precip.sel(time='2020-10-02', latitude=slice(latm,latM)) \n\n# Define a normalization instance for the colorbar (not used here)\nnorm1 = mcolors.TwoSlopeNorm(vmin=0, vcenter=131.45/2, vmax=131.45)\n\nfig = plt.figure()\nax = plt.axes(projection = projPC)\nim = ax.pcolormesh(gpcp_xtrem_precip_20201002.longitude, gpcp_xtrem_precip_20201002.latitude, gpcp_xtrem_precip_20201002.values)#, norm=norm1)\nax.set_extent([lonm, lonM, latm, latM], crs=projPC)\ngl = ax.gridlines(draw_labels=True, linewidth=0, alpha=0.5)\n\nax.coastlines()          \nax.add_feature(cfeature.LAND)\nax.add_feature(cfeature.COASTLINE)\nax.add_feature(cfeature.LAKES, alpha=0.5)\nax.add_feature(cfeature.RIVERS)\ncbar = plt.colorbar(im, label='precipitation (mm/d)',  extend='max', anchor=(0.2, 0.5), shrink=0.6, aspect=20) \nax.set_title('Total precipitation (mm) on the south-east of France on the 02/10/2020 - from GPCP dataset')\n\nera_xtrem_precip_20201002 = 3600.*era_region_precip.sel(time='2020-10-02')[0] \n\n# Define a normalization instance for the colorbar (not used here)\nnorm2 = mcolors.TwoSlopeNorm(vmin=0, vcenter=131.45/2, vmax=131.45)\n\nfig = plt.figure()\nax = plt.axes(projection = projPC)\nim = ax.pcolormesh(era_xtrem_precip_20201002.longitude, era_xtrem_precip_20201002.latitude, era_xtrem_precip_20201002.values)#, norm=norm2)\nax.set_extent([lonm, lonM, latm, latM], crs=projPC)\ngl = ax.gridlines(draw_labels=True, linewidth=0, alpha=0.5)\n\nax.coastlines()          \nax.add_feature(cfeature.LAND)\nax.add_feature(cfeature.COASTLINE)\nax.add_feature(cfeature.LAKES, alpha=0.5)\nax.add_feature(cfeature.RIVERS)\ncbar = plt.colorbar(im, label='precipitation (mm/d)',  extend='max', anchor=(0.2, 0.5), shrink=0.6, aspect=20)  # ajout de la colorbar\nax.set_title('Total precipitation (mm) on the south-east of France on the 02/10/2020 - from ERA5 dataset')\n\n","type":"content","url":"/notebooks/openchallengenotebook-c1-jessel-hezzi-roussel#id-9-4-maps-of-the-daily-precipitation-for-the-three-datasets-on-that-particular-day","position":59},{"hierarchy":{"lvl1":"9. Analysis cells ","lvl3":"9.5) Quantity and location of the maximum of precipitation (+ refined time-serie around the event)","lvl2":">>> We select the 2nd of october 2020 (detected by GPM-CPC and GPCP)"},"type":"lvl3","url":"/notebooks/openchallengenotebook-c1-jessel-hezzi-roussel#id-9-5-quantity-and-location-of-the-maximum-of-precipitation-refined-time-serie-around-the-event","position":60},{"hierarchy":{"lvl1":"9. Analysis cells ","lvl3":"9.5) Quantity and location of the maximum of precipitation (+ refined time-serie around the event)","lvl2":">>> We select the 2nd of october 2020 (detected by GPM-CPC and GPCP)"},"content":"\n\n","type":"content","url":"/notebooks/openchallengenotebook-c1-jessel-hezzi-roussel#id-9-5-quantity-and-location-of-the-maximum-of-precipitation-refined-time-serie-around-the-event","position":61},{"hierarchy":{"lvl1":"9. Analysis cells ","lvl4":"9.5.1) On the hydrology cube","lvl3":"9.5) Quantity and location of the maximum of precipitation (+ refined time-serie around the event)","lvl2":">>> We select the 2nd of october 2020 (detected by GPM-CPC and GPCP)"},"type":"lvl4","url":"/notebooks/openchallengenotebook-c1-jessel-hezzi-roussel#id-9-5-1-on-the-hydrology-cube","position":62},{"hierarchy":{"lvl1":"9. Analysis cells ","lvl4":"9.5.1) On the hydrology cube","lvl3":"9.5) Quantity and location of the maximum of precipitation (+ refined time-serie around the event)","lvl2":">>> We select the 2nd of october 2020 (detected by GPM-CPC and GPCP)"},"content":"\n\n# maximum value and index of latitude and longitude of the maximum\n\nprint('FOR GPM-CPC')\nprint('maximum value: (mm/d)', np.nanmax(hydro_xtrem_precip_20201002))\nprint('index for latitude and longitude of the maximum: ', np.where(hydro_xtrem_precip_20201002 == np.nanmax(hydro_xtrem_precip_20201002)))\nhydro_xtrem_precip_20201002.isel(lat=446, lon=1487) \n\n### time serie around the 2nd of october 2020, at the grid cell of the maximum value\n\nlat_hydro=44.16 ; lon_hydro=7.687\n\nprecip_point_hydro = precip_data_hydro.sel(lat=lat_hydro, lon=lon_hydro, method='nearest')\nprecip_point_hydro1 = precip_point_hydro.sel(time=slice('2020-09-30', '2020-10-06'))\n\nplt.figure(figsize=(8, 4))\nplt.bar(precip_point_hydro1.time.values, precip_point_hydro1.values, width=0.1, color='blue', align='center')\nplt.title('Precipitation from GPM CPC SM2RAIN-ASCAT (hydrology cube)')\nplt.xlabel('Day')\nplt.ylabel('Daily rainfall (mm/d)')\nplt.xticks(rotation=45, ha='right')\nplt.grid(True)\nplt.tight_layout()\nplt.show()\n\n","type":"content","url":"/notebooks/openchallengenotebook-c1-jessel-hezzi-roussel#id-9-5-1-on-the-hydrology-cube","position":63},{"hierarchy":{"lvl1":"9. Analysis cells ","lvl4":"9.5.2) On the GPCP cube","lvl3":"9.5) Quantity and location of the maximum of precipitation (+ refined time-serie around the event)","lvl2":">>> We select the 2nd of october 2020 (detected by GPM-CPC and GPCP)"},"type":"lvl4","url":"/notebooks/openchallengenotebook-c1-jessel-hezzi-roussel#id-9-5-2-on-the-gpcp-cube","position":64},{"hierarchy":{"lvl1":"9. Analysis cells ","lvl4":"9.5.2) On the GPCP cube","lvl3":"9.5) Quantity and location of the maximum of precipitation (+ refined time-serie around the event)","lvl2":">>> We select the 2nd of october 2020 (detected by GPM-CPC and GPCP)"},"content":"\n\n# maximum value and index of latitude and longitude of the maximum\n\nprint('FOR GPCP')\nprint('maximum value: (mm/d)', np.nanmax(gpcp_xtrem_precip_20201002))\nprint('index for latitude and longitude of the maximum: ', np.where(gpcp_xtrem_precip_20201002 == np.nanmax(gpcp_xtrem_precip_20201002)))\n\ngpcp_xtrem_precip_20201002.isel(latitude=2, longitude=5) \n\n### time serie around the 2nd of october 2020, at the grid cell of the maximum value\n\nlat_gpcp= 45 ; lon_gpcp= 8\n\nprecip_point_gpcp = precip_data_gpcp.sel(latitude=lat_gpcp, longitude=lon_gpcp, method='nearest')\nprecip_point_gpcp1 = precip_point_gpcp.sel(time=slice('2020-09-30', '2020-10-06'))\n\nplt.figure(figsize=(8, 4))\nplt.bar(precip_point_gpcp1.time.values, precip_point_gpcp1.values, width=0.1, color='blue', align='center')\nplt.title('Precipitation from GPCP')\nplt.xlabel('Day')\nplt.ylabel('Daily rainfall (mm/d)')\nplt.xticks(rotation=45, ha='right')\nplt.grid(True)\nplt.tight_layout()\nplt.show()\n\n","type":"content","url":"/notebooks/openchallengenotebook-c1-jessel-hezzi-roussel#id-9-5-2-on-the-gpcp-cube","position":65},{"hierarchy":{"lvl1":"9. Analysis cells ","lvl4":"9.5.3) On the ERA5 cube","lvl3":"9.5) Quantity and location of the maximum of precipitation (+ refined time-serie around the event)","lvl2":">>> We select the 2nd of october 2020 (detected by GPM-CPC and GPCP)"},"type":"lvl4","url":"/notebooks/openchallengenotebook-c1-jessel-hezzi-roussel#id-9-5-3-on-the-era5-cube","position":66},{"hierarchy":{"lvl1":"9. Analysis cells ","lvl4":"9.5.3) On the ERA5 cube","lvl3":"9.5) Quantity and location of the maximum of precipitation (+ refined time-serie around the event)","lvl2":">>> We select the 2nd of october 2020 (detected by GPM-CPC and GPCP)"},"content":"\n\nprint('FOR ERA5')\nprint('maximum value: (mm/d)', np.nanmax(era_xtrem_precip_20201002))\nprint('index for latitude and longitude of the maximum: ', np.where(era_xtrem_precip_20201002 == np.nanmax(era_xtrem_precip_20201002)))\n\nlat_era= 44; lon_era= 7\n\nprecip_point_era = 3600.0*era_cube['tp'].sel(latitude=lat_era, longitude=lon_era, method='nearest')\nprecip_point_era1 = precip_point_era.sel(time=slice('2020-09-30', '2020-10-06'))\n\nplt.figure(figsize=(8, 4))\nplt.bar(precip_point_era1.time.values, precip_point_era1.values, width=0.1, color='blue', align='center')\nplt.title('Precipitation from ERA5')\nplt.xlabel('Day')\nplt.ylabel('Daily rainfall (mm/d)')\nplt.xticks(rotation=45, ha='right')\nplt.grid(True)\nplt.tight_layout()\nplt.show()","type":"content","url":"/notebooks/openchallengenotebook-c1-jessel-hezzi-roussel#id-9-5-3-on-the-era5-cube","position":67},{"hierarchy":{"lvl1":"Monitoring socio-economic activities with nighttime light maps"},"type":"lvl1","url":"/notebooks/nightlights-notebook/night-lights-blending","position":0},{"hierarchy":{"lvl1":"Monitoring socio-economic activities with nighttime light maps"},"content":"","type":"content","url":"/notebooks/nightlights-notebook/night-lights-blending","position":1},{"hierarchy":{"lvl1":"Monitoring socio-economic activities with nighttime light maps"},"type":"lvl1","url":"/notebooks/nightlights-notebook/night-lights-blending#monitoring-socio-economic-activities-with-nighttime-light-maps","position":2},{"hierarchy":{"lvl1":"Monitoring socio-economic activities with nighttime light maps"},"content":"","type":"content","url":"/notebooks/nightlights-notebook/night-lights-blending#monitoring-socio-economic-activities-with-nighttime-light-maps","position":3},{"hierarchy":{"lvl1":"Monitoring socio-economic activities with nighttime light maps","lvl2":"Case study: Creating nighttime light maps with color blending"},"type":"lvl2","url":"/notebooks/nightlights-notebook/night-lights-blending#case-study-creating-nighttime-light-maps-with-color-blending","position":4},{"hierarchy":{"lvl1":"Monitoring socio-economic activities with nighttime light maps","lvl2":"Case study: Creating nighttime light maps with color blending"},"content":"\n\nThis notebook demonstrates how to create nighttime light variation maps over a time interval (from 2019 to 2022) with additive color blending for studying the distribution of artificial lighting in urban and rural regions, offering insights into urbanization, infrastructure development, and socio-economic activity.\nThe Nighttime Light Levels indicator can be explored on the EO Dashboard by selecting the EXPLORE DATASETS mode and choosing \n\nNight lights indicators.\n\nThe study has been carried out by Bumpei Tojo PhD (Associate Professor), School of International and Area Studies, Tokyo University of Foreign Studies, Japan.\n\nThe PLES Engineering team (supporting ESA Green Solutions Division, EOP-SG) developed the blending algorithm with the current Notebook implementation and data ingestion on the dashboard (contacts: Federico Rondoni, \n\nf​.rondoni@stariongroup​.eu, Diego Moglioni, \n\nd​.moglioni@stariongroup​.eu).\n\n\nExample of a nighttime light map covering Northeastern United States during 2019-2022, representing areas where a decrease in nighttime light level occurred (indicating a presumed reduction in social activity) in red, and areas where an increase occurred (indicating a presumed increase in social activity) in blue.\n\n","type":"content","url":"/notebooks/nightlights-notebook/night-lights-blending#case-study-creating-nighttime-light-maps-with-color-blending","position":5},{"hierarchy":{"lvl1":"Monitoring socio-economic activities with nighttime light maps","lvl2":"Input data:"},"type":"lvl2","url":"/notebooks/nightlights-notebook/night-lights-blending#input-data","position":6},{"hierarchy":{"lvl1":"Monitoring socio-economic activities with nighttime light maps","lvl2":"Input data:"},"content":"This product was developed by JAXA by aggregating the data from the Visible Infrared Imaging Radiometer Suite (VIIRS, onboard the Suomi NPP satellite) on a semi-annual basis (using median image generation) with 10-degree lat/lon (geographic) grid h-v tile as Geo Tiff files.\n\nThe tiles are 2400 x 2400 pixels, with pixel size of 0.004166 degrees (~464 m at the equator).\n\nMetric: nighttime radiance (measured in watts per square centimeter per steradian, [W/cm²/sr]), which represents the brightness of artificial lighting.\n\nVIIRS 10-degree tile scheme\n\n","type":"content","url":"/notebooks/nightlights-notebook/night-lights-blending#input-data","position":7},{"hierarchy":{"lvl1":"Monitoring socio-economic activities with nighttime light maps","lvl2":"Running environment:"},"type":"lvl2","url":"/notebooks/nightlights-notebook/night-lights-blending#running-environment","position":8},{"hierarchy":{"lvl1":"Monitoring socio-economic activities with nighttime light maps","lvl2":"Running environment:"},"content":"This notebook can be run by installing the provided Anaconda Python environment (data/Nightlights/nightlights-env.yml).\n\n","type":"content","url":"/notebooks/nightlights-notebook/night-lights-blending#running-environment","position":9},{"hierarchy":{"lvl1":"Monitoring socio-economic activities with nighttime light maps","lvl2":"Importing Python libraries"},"type":"lvl2","url":"/notebooks/nightlights-notebook/night-lights-blending#importing-python-libraries","position":10},{"hierarchy":{"lvl1":"Monitoring socio-economic activities with nighttime light maps","lvl2":"Importing Python libraries"},"content":"\n\n#Import libraries\nimport os\nimport sys\nimport re\nimport numpy as np\nimport rasterio\nimport rioxarray as rxr\nimport subprocess\nfrom subprocess import Popen, PIPE\nimport tempfile\nimport matplotlib.pyplot as plt\nimport matplotlib.image as mpimg\nfrom PIL import Image\nimport json\nfrom ipyleaflet import Map, ImageOverlay, basemaps\n\n","type":"content","url":"/notebooks/nightlights-notebook/night-lights-blending#importing-python-libraries","position":11},{"hierarchy":{"lvl1":"Monitoring socio-economic activities with nighttime light maps","lvl2":"Defining working folders"},"type":"lvl2","url":"/notebooks/nightlights-notebook/night-lights-blending#defining-working-folders","position":12},{"hierarchy":{"lvl1":"Monitoring socio-economic activities with nighttime light maps","lvl2":"Defining working folders"},"content":"\n\ninput_dir = \"input\"\ninput_path = os.path.join(os.getcwd(), \"data\", \"Nightlights\", input_dir)\n\noutput_urban = \"output\"\nurban_path = os.path.join(os.getcwd(), \"data\", \"Nightlights\", output_urban)\n\n# Create output dir\nos.makedirs(urban_path, exist_ok=True)\n\n","type":"content","url":"/notebooks/nightlights-notebook/night-lights-blending#defining-working-folders","position":13},{"hierarchy":{"lvl1":"Monitoring socio-economic activities with nighttime light maps","lvl2":"Visualizing input data"},"type":"lvl2","url":"/notebooks/nightlights-notebook/night-lights-blending#visualizing-input-data","position":14},{"hierarchy":{"lvl1":"Monitoring socio-economic activities with nighttime light maps","lvl2":"Visualizing input data"},"content":"\n\npattern = re.compile(r\"h(\\d+)v(\\d+)_(\\d{4})_(\\d)H_median\\.tif\")\n\nfor root, _, files in os.walk(input_path):\n    grouped_files = {}\n    \n    # Grouping files by h and v\n    for file in files:\n        match = pattern.match(file)\n        if match:\n            h, v, year, H = match.groups()\n            key = (h, v)\n            if key not in grouped_files:\n                grouped_files[key] = []\n            grouped_files[key].append((year, H, os.path.join(root, file))) \n    \n    # Process files for plotting\n    for (h, v), file_group in grouped_files.items():\n        file_group\n\ntuplelist2dict = {c:{'year':a,'half':b} for a,b,c in file_group}\nsorted_dict = {}\n\ngrid_tile = None\nfor item in sorted(tuplelist2dict, key = lambda k: (tuplelist2dict[k]['year'], tuplelist2dict[k]['half'])):\n    sorted_dict.update({item:tuplelist2dict[item]})\n    grid_tile = item.split('/')[-1].split('_')[0]\n\n\ndef plot_figures(figures, nrows = 1, ncols=1, factor = 1., clip_range = None, **kwargs):\n    fig, axeslist = plt.subplots(ncols=ncols, nrows=nrows, figsize=(15, 15))\n    for idx,file_name in enumerate(figures):\n        title = file_name.split('/')[-1]\n        title = title.replace(\"_median.tif\", \"\")\n        axeslist.ravel()[idx].imshow(np.clip(mpimg.imread(file_name)*factor, *clip_range), **kwargs) # cmap=plt.gray(), cmap='Greys'\n        axeslist.ravel()[idx].set_title(title)\n        axeslist.ravel()[idx].set_axis_off()\n    fig.suptitle(r'Median nighttime light level [$Wcm^{-2}sr^{-1}$]', fontsize=20)\n    #plt.tight_layout() \n\n    \n\nplot_figures(sorted_dict, 4, 2, clip_range=(0,1), factor=100./255.)\n\n","type":"content","url":"/notebooks/nightlights-notebook/night-lights-blending#visualizing-input-data","position":15},{"hierarchy":{"lvl1":"Monitoring socio-economic activities with nighttime light maps","lvl2":"Applying color blending"},"type":"lvl2","url":"/notebooks/nightlights-notebook/night-lights-blending#applying-color-blending","position":16},{"hierarchy":{"lvl1":"Monitoring socio-economic activities with nighttime light maps","lvl2":"Applying color blending"},"content":"\n\nAdditive color blending involves mixing the primary colors of light, red (R), green (G), and blue (B), while adjusting their luminance levels. By manipulating the luminance of these colors, a wide range of hues can be achieved, with the key characteristic being that combining all three colors results in white.\n\nTechnique\n\nmaximum pixel value of images for 2019 (brightest pixel values) is assigned to red (R)\n\nminimum pixel value of images for 2020-2021 (darkest pixel values) is assigned to green (G)\n\nmaximum pixel value of images for 2022 (brightest pixel values) is assigned to blue (B)\n\nOutput\n\nregions where a decrease in nighttime light level occurred are displayed in red\n\nareas that experienced an increase (presumed to indicate an increase in social activity) are shown in blue\n\nregions where there was no significant change throughout the pandemic are displayed in a gradient from gray to white.\n\npattern = re.compile(r\"h(\\d+)v(\\d+)_(\\d{4})_(\\d)H_median\\.tif\")\n\n\ndef read_and_preprocess(file):\n    with rasterio.open(file) as src:\n        img = src.read(1)\n        img = np.nan_to_num(img, nan=0)  # Replace NaNs with zeros\n        return img, src.profile\n\n    \ndef additive_blending(grouped_files):\n    # Process each group of files for blending\n    for (h, v), file_group in grouped_files.items():\n        # Initialize arrays to store the maximum and minimum values for each year\n        max_2019 = None\n        min_2020_2021 = None\n        max_2022 = None\n        \n        for year, H, file in file_group:\n            img, profile = read_and_preprocess(file)\n            \n            if year == '2019':\n                if max_2019 is None:\n                    max_2019 = img\n                else:\n                    max_2019 = np.maximum(max_2019, img)\n            elif year in ['2020', '2021']:\n                if min_2020_2021 is None:\n                    min_2020_2021 = img\n                else:\n                    min_2020_2021 = np.minimum(min_2020_2021, img)\n            elif year == '2022':\n                if max_2022 is None:\n                    max_2022 = img\n                else:\n                    max_2022 = np.maximum(max_2022, img)\n                                  \n    # Create the blended image using the specified blending technique\n    blended_img = np.zeros((max_2019.shape[0], max_2019.shape[1], 3), dtype=np.float32)\n    blended_img[:, :, 0] = max_2019.astype(np.float32)  # Red band: maximum of 2019\n    blended_img[:, :, 1] = min_2020_2021.astype(np.float32)  # Green band: minimum of 2020-2021 compared\n    blended_img[:, :, 2] = max_2022.astype(np.float32)  # Blue band: maximum of 2022\n    \n    return blended_img, profile        \n\n# Grouping files per (h,v) tiles and years span\ngrouped_files = {}\nfor root, _, files in os.walk(input_path):\n    \n    \n    # Group files by h and v\n    for file in files:\n        match = pattern.match(file)\n        if match:\n            h, v, year, H = match.groups()\n            key = (h, v)\n            if key not in grouped_files:\n                grouped_files[key] = []\n            grouped_files[key].append((year, H, os.path.join(root, file)))\n\n\n# blending           \nblended_img, profile = additive_blending(grouped_files)\n\n","type":"content","url":"/notebooks/nightlights-notebook/night-lights-blending#applying-color-blending","position":17},{"hierarchy":{"lvl1":"Monitoring socio-economic activities with nighttime light maps","lvl2":"Employing “urban” stretch processing"},"type":"lvl2","url":"/notebooks/nightlights-notebook/night-lights-blending#employing-urban-stretch-processing","position":18},{"hierarchy":{"lvl1":"Monitoring socio-economic activities with nighttime light maps","lvl2":"Employing “urban” stretch processing"},"content":"\n\nThe purpose of the stretch processing is to emphasize the nighttime light level of each band (corresponding to 2019, 2020-21, and 2022), focusing on:\n\n“urban” nighttime light levels: the display range of each band is adjusted to 25-1000 [watts·cm-2·sr-1], emphasizing brighter areas\n\n# Urban stretch processing: Emphasize brighter areas\ndef urban_stretch_processing(band):\n    min_val = 25\n    max_val = 1000\n    band = np.clip(band, min_val, max_val)\n    band = (band - min_val) / (max_val - min_val) * 255\n    return band.astype(np.uint8)\n\n# Apply urban stretch processing to each band\nblended_img[:, :, 0] = urban_stretch_processing(blended_img[:, :, 0])\nblended_img[:, :, 1] = urban_stretch_processing(blended_img[:, :, 1])\nblended_img[:, :, 2] = urban_stretch_processing(blended_img[:, :, 2])\n        \n# Update the profile for a 3-band image and set nodata value to None\nprofile.update(count=3, dtype=rasterio.uint8, nodata=None)\n\n","type":"content","url":"/notebooks/nightlights-notebook/night-lights-blending#employing-urban-stretch-processing","position":19},{"hierarchy":{"lvl1":"Monitoring socio-economic activities with nighttime light maps","lvl2":"Plotting blended image"},"type":"lvl2","url":"/notebooks/nightlights-notebook/night-lights-blending#plotting-blended-image","position":20},{"hierarchy":{"lvl1":"Monitoring socio-economic activities with nighttime light maps","lvl2":"Plotting blended image"},"content":"\n\ndef plot_image(image, factor=1.0, clip_range = None, **kwargs):\n    fig, ax = plt.subplots(nrows=1, ncols=1, figsize=(15, 15))\n    if clip_range is not None:\n        ax.imshow(np.clip(image * factor, *clip_range), **kwargs)\n    else:\n        ax.imshow(image * factor, **kwargs)\n    ax.set_xticks([])\n    ax.set_yticks([])\n\n\nplot_image(blended_img, factor=1.5/255., clip_range=(0,1))\n\n","type":"content","url":"/notebooks/nightlights-notebook/night-lights-blending#plotting-blended-image","position":21},{"hierarchy":{"lvl1":"Monitoring socio-economic activities with nighttime light maps","lvl2":"Overlaying with an interactive map"},"type":"lvl2","url":"/notebooks/nightlights-notebook/night-lights-blending#overlaying-with-an-interactive-map","position":22},{"hierarchy":{"lvl1":"Monitoring socio-economic activities with nighttime light maps","lvl2":"Overlaying with an interactive map"},"content":"\n\nSaving blended image as Cloud Optimized GeoTIFF\n\n# Exporting output as COG file \nres_name = f\"Nighttimelevel_Urban_{grid_tile}_2019-2022.tif\"\noutput_cog = os.path.join(urban_path, res_name)\n\n\n# Write the blended image directly to a temporary file\nwith tempfile.NamedTemporaryFile(suffix=\".tif\") as temp_file:\n    with rasterio.open(temp_file.name, \"w\", **profile) as dst:\n        dst.write(blended_img[:, :, 0], 1)\n        dst.write(blended_img[:, :, 1], 2)\n        dst.write(blended_img[:, :, 2], 3)\n\n    subprocess.run([\n        \"gdal_translate\", \"-of\", \"COG\",\n        \"-co\", \"COMPRESS=DEFLATE\",\n        \"-co\", \"BLOCKSIZE=512\",\n        \"-co\", \"RESAMPLING=NEAREST\",\n        \"-co\", \"OVERVIEWS=IGNORE_EXISTING\",\n        temp_file.name,\n        output_cog\n    ])\n\nprint(f\"COG file saved as {output_cog}\")\n\nExtracting bounding-box coordinates\n\np1 = Popen([\"gdalinfo\", \"-json\", output_cog, \"-mm\"], stdout=PIPE)\noutput = p1.communicate()[0]\ndec_out = output.decode('utf-8')\nj_str = json.loads(dec_out)\nbbox = j_str['cornerCoordinates']\nll = bbox['lowerLeft']  # SW coo\nur = bbox['upperRight'] # NE coo\ncenter = bbox['center']\n\nAdding alpha channel to the blended image\n\ndef almostEquals(a,b,thres=50):\n    return all(abs(a[i]-b[i])<thres for i in range(len(a)))\n\n\nimage = Image.open(output_cog).convert('RGBA')\npixeldata = list(image.getdata())\n\n\nfor i,pixel in enumerate(pixeldata):\n    if almostEquals(pixel[:3], (0,0,0)):\n        pixeldata[i] = (0,0,0,0)\n\n\nimage.putdata(pixeldata)\npng_path = os.path.join(urban_path, res_name.replace(\".tif\",\".png\"))\nres = image.save(png_path)\n#os.path.abspath(res)\nabs_path = os.path.abspath(png_path)\nwork_dir = os.getcwd()\nid_ = work_dir.split('/')[2]\nbase_url = os.path.join('https://hub-otc-sc.eox.at/user',id_,'files')\n#base_url\nabs_path = '/'.join(abs_path.split('/')[3:])\n#abs_path\n#os.path.join(base_url,abs_path)\n\nOverlay with OpenStreetMap\n\nm = Map(basemap=basemaps.CartoDB.DarkMatter, center=(center[1], center[0]), zoom=6)\nimage = ImageOverlay(\n        url=os.path.join(base_url,abs_path), #url=\"https://hub-otc-sc.eox.at/user/<id>/files/<path_to_png>\n        bounds=((ll[1], ll[0]), (ur[1], ur[0])) # SW and NE corners\n                    )\nm.add_layer(image);\nm\n\nUsing additive color blending images to focus on urban areas in the northeastern United States, particularly the southern part of the Boston-Washington Corridor comprising major cities like New York, Philadelphia, Baltimore, and Washington, D.C., revealed relatively little variation in nighttime lights before and after the COVID-19 pandemic, as indicated by the images appearing white. However, significant reductions in nighttime lights during the COVID-19 pandemic (subsequently recovering to pre-pandemic levels) were evident in other medium to large cities, including Boston and Toronto, among others, as indicated by the images appearing pink to reddish.\n\n","type":"content","url":"/notebooks/nightlights-notebook/night-lights-blending#overlaying-with-an-interactive-map","position":23},{"hierarchy":{"lvl1":"Monitoring socio-economic activities with nighttime light maps","lvl2":"Alternative stretch processing approaches"},"type":"lvl2","url":"/notebooks/nightlights-notebook/night-lights-blending#alternative-stretch-processing-approaches","position":24},{"hierarchy":{"lvl1":"Monitoring socio-economic activities with nighttime light maps","lvl2":"Alternative stretch processing approaches"},"content":"\n\n“Rural” nighttime light levels: setting each band’s display range to 0-50 [Watts·cm-2·sr-1] to emphasize darker areas\n\nFor a broader focus (from rural to urban areas): applying a log10 transformation to the values of each band with a display range set to 0-3.6 (equivalent to 0-3981 [Watts·cm-2·sr-1])\n\n","type":"content","url":"/notebooks/nightlights-notebook/night-lights-blending#alternative-stretch-processing-approaches","position":25},{"hierarchy":{"lvl1":"Monitoring socio-economic activities with nighttime light maps","lvl2":"Final remarks"},"type":"lvl2","url":"/notebooks/nightlights-notebook/night-lights-blending#final-remarks","position":26},{"hierarchy":{"lvl1":"Monitoring socio-economic activities with nighttime light maps","lvl2":"Final remarks"},"content":"\n\nThis product has been specifically designed for utilization by researchers in applied scientific fields.\nPotential applications are being explored in environmental and social issue studies, such as disaster recovery, energy, urban land use changes, conflicts, migration, and monitoring of illegal, unreported activities.\n\n","type":"content","url":"/notebooks/nightlights-notebook/night-lights-blending#final-remarks","position":27},{"hierarchy":{"lvl1":"Monitoring socio-economic activities with nighttime light maps","lvl2":"To learn more"},"type":"lvl2","url":"/notebooks/nightlights-notebook/night-lights-blending#to-learn-more","position":28},{"hierarchy":{"lvl1":"Monitoring socio-economic activities with nighttime light maps","lvl2":"To learn more"},"content":"\n\nThe presented approach is often referenced in scientific and public health research. For example, Bunpei Tojo (2021) used Nightlights data in the study “Application of Earth observation satellite data to public health: Comparison of night light (VIIRS) and solar radiation (SGLI) and domestic COVID-19 epidemic,” presented at the 12th Federation of Science and Technology conference, to analyze the relationship between nighttime light intensity and public health factors.\n\n\n東城 (2021).","type":"content","url":"/notebooks/nightlights-notebook/night-lights-blending#to-learn-more","position":29},{"hierarchy":{"lvl1":"Using the NASA VEDA EOAPI"},"type":"lvl1","url":"/notebooks/veda-api-bids-2023","position":0},{"hierarchy":{"lvl1":"Using the NASA VEDA EOAPI"},"content":"This notebook is divided into two parts, each demonstrating the functionalities of the VEDA EOAPI.\n\nReading and visualizing one of the datasets from the VEDA data catalog.\n\nUsing the EOAPI to generate a time-series of OMI (Ozone Monitoring Instrument) NO₂ and SO₂ datasets\n\nAuthor: Slesa Adhikari\n\n","type":"content","url":"/notebooks/veda-api-bids-2023","position":1},{"hierarchy":{"lvl1":"Using the NASA VEDA EOAPI","lvl2":"1. Reading and visualizing one of the datasets from the VEDA data catalog"},"type":"lvl2","url":"/notebooks/veda-api-bids-2023#id-1-reading-and-visualizing-one-of-the-datasets-from-the-veda-data-catalog","position":2},{"hierarchy":{"lvl1":"Using the NASA VEDA EOAPI","lvl2":"1. Reading and visualizing one of the datasets from the VEDA data catalog"},"content":"\n\nImport all the necesssary libraries\n\nMake sure you install these first using:pip install pystac_client folium seaborn pandas\n\n# imports\nimport requests\nfrom pystac_client import Client\nimport folium\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport pandas as pd\n\nDefine the API endpoints\n\nThe EOAPI is a combination of two components.\n\nData catalog - the \n\nSpatioTemporal Asset Catalog (STAC) specification is used to catalog the available datasets\n\nDynamic tile server - \n\nTiTiler is used to dynamically serve cloud optimized geotiff (raster) data files\n\nSTAC_API_URL = \"https://staging-stac.delta-backend.com/\"\nRASTER_API_URL = \"https://staging-raster.delta-backend.com\"\n\nUse the pystac_client library to interact with the STAC data catalog\n\ncatalog = Client.open(STAC_API_URL)\n\nList all the datasets (collections) in the catalog\n\nfor collection in list(catalog.get_collections()):\n    print(f\"{collection.id} - {collection.title}\")\n\nChoose a collection to work with\n\nSearch all the items in the collection\n\ncollection_id = \"no2-monthly\"\nsearch = catalog.search(collections=[collection_id])\nitems = list(search.items())\nitems\n\nLoad and inspect one of the items\n\ns3_uri = items[0].assets[\"cog_default\"].href\n\nstats = requests.get(\n    f\"{RASTER_API_URL}/cog/statistics\",\n    params={\"url\": s3_uri}\n).json()\nstats\n\nDisplay the COG in a map\n\nGet the tiles endpoint for the file\n\nrescale = f\"{stats['b1']['min']},{stats['b1']['max']}\"\n\ntiles = requests.get(\n    f\"{RASTER_API_URL}/stac/tilejson.json?collection={collection_id}&item={items[0].id}&assets=cog_default&colormap_name=rdbu_r&rescale={rescale}\"\n).json()\n\ntiles\n\nUse the tiles to visualize the file in a map\n\nm = folium.Map(\n    zoom_start=6,\n    scroll_wheel_zoom=True, \n    tiles=tiles[\"tiles\"][0], \n    attr=\"VEDA\", \n    minzoom=0, \n    maxzoom=18,\n)\n\nm\n\nMake this map slightly more insightful\n\nUsing the minimum and maximum values for the colorscale may not be the most useful. Dynamic tiling to the rescue!\nTiTiler comes with a \n\nlarge amount of options to style the map. Here we’ll do a quick a dirty adjustment of the max value.\n\nrescale_limited_max = f\"{stats['b1']['min']},{stats['b1']['max']/2}\"\n\ntiles_limited_max = requests.get(\n    f\"{RASTER_API_URL}/stac/tilejson.json?collection={collection_id}&item={items[0].id}&assets=cog_default&colormap_name=rdbu_r&rescale={rescale_limited_max}\"\n).json()\ntiles_limited_max\n\nm_limited_max = folium.Map(\n    zoom_start=6,\n    scroll_wheel_zoom=True, \n    tiles=tiles_limited_max[\"tiles\"][0], \n    attr=\"VEDA\", \n    minzoom=0, \n    maxzoom=18,\n)\nm_limited_max\n\n","type":"content","url":"/notebooks/veda-api-bids-2023#id-1-reading-and-visualizing-one-of-the-datasets-from-the-veda-data-catalog","position":3},{"hierarchy":{"lvl1":"Using the NASA VEDA EOAPI","lvl2":"2. Using the EOAPI to generate a time-series of OMI (Ozone Monitoring Instrument) NO₂ and SO₂ datasets"},"type":"lvl2","url":"/notebooks/veda-api-bids-2023#id-2-using-the-eoapi-to-generate-a-time-series-of-omi-ozone-monitoring-instrument-no-and-so-datasets","position":4},{"hierarchy":{"lvl1":"Using the NASA VEDA EOAPI","lvl2":"2. Using the EOAPI to generate a time-series of OMI (Ozone Monitoring Instrument) NO₂ and SO₂ datasets"},"content":"\n\nThere’s a story published in the EODashboard with the following title:\n\nAir pollution in India, China and the U.S. have changed significantly over the past two decades\n\nLink: \n\nhttps://​eodashboard​.org​/story​?id​=​air​-pollution​-us​-india​-china\n\nThe story talks about the trend of air pollution in India, China and the U.S. using the NO2 and SO2 readings grabbed from the OMI instrument\n\nHere, we’ll recreate the analysis using the EOAPI\n\n# Here, we find the relevant collection ID for the dataset\ncollections = {\n    \"no2\": \"OMI_trno2-COG\",\n    \"so2\": \"OMSO2PCA-COG\",\n}\n\nDefine the roughly similar Area of Interest (AOI) for each of the country as seen in the story\n\naois = {\n    \"india\": {\n      \"type\": \"Feature\",\n      \"properties\": {},\n      \"geometry\": {\n        \"coordinates\": [\n          [\n            [\n              84.44227371801799,\n              25.276852788244952\n            ],\n            [\n              81.73331688510166,\n              25.379576397063317\n            ],\n            [\n              81.40290450746915,\n              20.640781701865322\n            ],\n            [\n              84.09079123546121,\n              20.59296261766137\n            ],\n            [\n              84.44227371801799,\n              25.276852788244952\n            ]\n          ]\n        ],\n        \"type\": \"Polygon\"\n      }\n    },\n    \"china\": {\n      \"type\": \"Feature\",\n      \"properties\": {},\n      \"geometry\": {\n        \"coordinates\": [\n          [\n            [\n              118.14487188674968,\n              40.38237805885373\n            ],\n            [\n              112.59679754686567,\n              40.39197699341523\n            ],\n            [\n              112.78712023622006,\n              32.015052150835814\n            ],\n            [\n              117.937454307721,\n              32.102440507249895\n            ],\n            [\n              118.14487188674968,\n              40.38237805885373\n            ]\n          ]\n        ],\n        \"type\": \"Polygon\"\n      }\n    },\n    \"usa\": {\n        \"type\": \"Feature\",\n        \"properties\": {},\n        \"geometry\": {\n            \"coordinates\": [\n            [\n                [\n                -80.16702521343733,\n                41.73420113945659\n                ],\n                [\n                -83.56446680395005,\n                38.599369254919566\n                ],\n                [\n                -82.00280661075571,\n                37.54658260550103\n                ],\n                [\n                -78.28140359718638,\n                40.450899619800595\n                ],\n                [\n                -80.16702521343733,\n                41.73420113945659\n                ]\n            ]\n            ],\n            \"type\": \"Polygon\"\n        }\n    },\n}\n\nDefine a function that takes the following params:\n\nitem: a STAC item\n\ngeojson: the geojson of the AOI\n\nUsing the /cog/statistics/ endpoint of the raster API, we get back the statistics of the item (which corresponds to one COG file) within the given geojson AOI.\n\nThe statistics includes min, max, mean, std, etc.\n\n# the bounding box should be passed to the geojson param as a geojson Feature or FeatureCollection\ndef generate_stats(item, geojson):\n    result = requests.post(\n        f\"{RASTER_API_URL}/cog/statistics\", \n        params={\n            \"url\": item.assets[\"cog_default\"].href\n        },\n        json=geojson\n    ).json()\n    return {\n        **result[\"properties\"],\n        \"start_datetime\": str(item.properties.get(\"datetime\", item.properties.get(\"start_datetime\")))[:4],\n        \"collection\": item.collection_id\n    }\n\nLet’s start out with the US 🇺🇸 !\n\nWe’ll get all the items in the NO2 and SO2 collections and generate the statistics from them for the Ohio River Valley region of the United States.\n\nusa_aoi = aois[\"usa\"]\nitems = list(catalog.search(collections=[collections[\"no2\"], collections[\"so2\"]]).items())\nusa_stats = [\n    generate_stats(item, usa_aoi)\n    for item in items\n]\n\nCreate a function that takes the statistics (which is a json) and converts it to a pandas dataframe in a format that’ll make it easy to read and visualize.\n\nWe’re only concerned with the mean statistics for this example. Specifically the change from the year 2005 in percentage. We’ll use pandas to calculate this change percentage and assign it to the change column.\n\ndef clean_stats(stats_json) -> pd.DataFrame:\n    # convert the stats_json as is to pandas dataframe\n    df = pd.json_normalize(stats_json)\n    # simple renaming for readability\n    df.columns = [col.replace(\"statistics.b1.\", \"\") for col in df.columns]\n    # create a date column from the start_datetime column\n    df[\"date\"] = pd.to_datetime(df[\"start_datetime\"], format=\"%Y\")\n    # sort the dataframe by the date column\n    df = df.sort_values(by=[\"date\"])\n    # create a change column that calculates the change of the mean values from the value in 2005\n    df[\"change\"] = df.groupby(\"collection\", group_keys=False)[\"mean\"].apply(lambda x: x.div(x.iloc[0]).subtract(1).mul(100))\n    return df\n\ncleaned_stats_df = clean_stats(usa_stats)\n\nWe’ll now create a time-series of the change in mean values for NO2 and SO2 for the area in the US.\n\nplt.xticks([i for i in range(0, 21, 2)])\nsns.set_style(\"darkgrid\")\nax = sns.lineplot(\n    x=\"start_datetime\",\n    y=\"change\",\n    hue=\"collection\",\n    data=cleaned_stats_df,\n    palette=[\"#2196f3\", \"#ff5722\"],\n    style=\"collection\",\n    markers=[\"*\", \"d\"]\n)\nax.set_title(\"US - Ohio River Valley\")\nax.set_xlabel(\"Years\")\nax.set_ylabel(\"Change from 2005 (%)\")\nplt.legend(frameon=False, ncol=3)\nplt.show()\n\nNow, let’s create a function that creates this trend graph, given the country.\n\ndef create_chart(country):\n    stats = [generate_stats(item, aois[country]) for item in list(catalog.search(collections=[collections[\"no2\"], collections[\"so2\"]]).items())]\n    df = clean_stats(stats)\n\n    # Create a chart using Seaborn\n    plt.xticks([i for i in range(0, 21, 2)])\n    sns.set_style(\"darkgrid\")\n    ax = sns.lineplot(\n        x=\"start_datetime\",\n        y=\"change\",\n        hue=\"collection\",\n        data=df,\n        palette=[\"#2196f3\", \"#ff5722\"],\n        style=\"collection\",\n        markers=[\"*\", \"d\"]\n    )\n    ax.set_title(country.title())\n    ax.set_xlabel(\"Years\")\n    ax.set_ylabel(\"Change from 2005 (%)\")\n    plt.legend(frameon=False, ncol=3)\n    plt.show()\n\nWe can use this function to create charts for the rest of the AOIs.\n\nIndia 🇮🇳\n\ncreate_chart(\"india\")\n\nChina 🇨🇳\n\ncreate_chart(\"china\")","type":"content","url":"/notebooks/veda-api-bids-2023#id-2-using-the-eoapi-to-generate-a-time-series-of-omi-ozone-monitoring-instrument-no-and-so-datasets","position":5}]}